''' å¤§æ¨¡å‹èŠå¤©çš„pipelineè°ƒç”¨
ä¸»è¦åŠŸèƒ½ï¼š
1. å‘½ä»¤è¡Œè°ƒç”¨å„ä¸ªæ¨¡å‹demo
2. åˆ©ç”¨fastapiä¸ºå¤§æ¨¡å‹æ­å»ºopenaiæ ¼å¼çš„serverå’Œclientè°ƒç”¨
    Implements API for LLM in OpenAI's format. (https://platform.openai.com/docs/api-reference/chat)
    Usage: python openai_api.py
    Visit http://localhost:8000/docs for documents.
3. webç•Œé¢å¿«é€Ÿæ­å»ºdemo(gradio+streamlit)

# TODO: è®¾ç½®return_states=Trueæ—¶å€™ï¼Œå—åˆ°build_promptå½±å“ï¼Œå¾ˆéš¾ä¿è¯promptå®Œå…¨å¤ç°
è¿™é‡Œé‡‡ç”¨æ·»åŠ self.generation_config['states']['last_token']ï¼Œæ˜¯å› ä¸ºæ¨ç†å®Œæˆå¯èƒ½æ˜¯å› ä¸ºåˆ°è¾¾max_lengthï¼Œæœªå¿…æ˜¯é‡åˆ°äº†eos
'''

import os
import torch
from typing import Union, Optional, List, Tuple, Literal, Dict
from bert4torch.pipelines.base import PipeLineBase
from bert4torch.snippets import (
    log_warn_once, 
    get_config_path, 
    log_free,
    log_info, 
    log_info_once,
    log_warn, 
    log_error,
    colorful,
    cuda_empty_cache,
    is_fastapi_available, 
    is_pydantic_available, 
    is_streamlit_available,
    is_package_available,
    has_chinese_char,
    add_start_docstrings,
    JsonConfig,
    NoopContextManager,
    sequence_padding,
    DottableDict
)
from packaging import version
import gc
import time
import json
from contextlib import asynccontextmanager
import threading
import re
import copy
from .conversation import Conversation


class NoneObject:
    def __init__(self, *args, **kwarg):
        pass


if is_fastapi_available():
    from fastapi import FastAPI, HTTPException, APIRouter, Depends
    from fastapi.security.http import HTTPAuthorizationCredentials, HTTPBearer
    from fastapi.middleware.cors import CORSMiddleware
else:
    class FastAPI: pass
    class HTTPAuthorizationCredentials: pass
    Depends, HTTPBearer = NoneObject, NoneObject

if is_pydantic_available():
    from pydantic import BaseModel, Field
else:
    BaseModel, Field = object, NoneObject


if is_streamlit_available():
    import streamlit as st
else:
    # é˜²æ­¢streamlitä¸å­˜åœ¨æ—¶å€™æŠ¥é”™
    import bert4torch.snippets as st
    st.cache_resource = st.delete_arguments


__all__ = [
    'Chat',
    'Glm',
    'Glm2',
    'Glm3',
    'Glm4',
    'InternLM',
    'InternLM2',
    'Qwen',
    'Qwen2',
    'LLaMA2',
    'LLaMA3',
    'ApplyChatTemplate',
    'Ziya',
    'ChineseLlamaAlpaca',
    'Belle',
    'Baichuan',
    'PretrainedTextContinuation'
    ]


# ä¸€äº›é€šç”¨çš„systemè¯æœ¯
SYSTEM_ZH = """ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººã€å°Šé‡ä»–äººã€è¯šå®çš„ä¸­æ–‡èŠå¤©åŠ©æ‰‹ã€‚åœ¨å®‰å…¨çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆå°½å¯èƒ½æä¾›å¸®åŠ©ã€‚ä½ çš„å›ç­”ä¸åº”åŒ…æ‹¬ä»»ä½•æœ‰å®³ã€ä¸é“å¾·ã€ç§æ—ä¸»ä¹‰ã€æ€§åˆ«æ­§è§†ã€æœ‰æ¯’ã€å±é™©æˆ–éæ³•çš„å†…å®¹ã€‚è¯·ç¡®ä¿ä½ çš„å›ç­”æ˜¯ç¤¾ä¼šå…¬æ­£å’Œç§¯æçš„ã€‚
å¦‚æœä¸€ä¸ªé—®é¢˜æ²¡æœ‰ä»»ä½•æ„ä¹‰ï¼Œæˆ–è€…äº‹å®ä¸Šä¸è¿è´¯ï¼Œè¯·è§£é‡ŠåŸå› ï¼Œè€Œä¸æ˜¯å›ç­”ä¸æ­£ç¡®çš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“é—®é¢˜çš„ç­”æ¡ˆï¼Œè¯·ä¸è¦åˆ†äº«è™šå‡ä¿¡æ¯ï¼Œæ‰€æœ‰å›ç­”å°½å¯èƒ½ä½¿ç”¨ä¸­æ–‡æ¥å›ç­”ã€‚
"""
SYSTEM_EN = """\
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.
If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\
"""

CHAT_START_DOCSTRING = r"""
    :param checkpoint_path: str, æ¨¡å‹æƒé‡åœ°å€ï¼Œå¯ä»¥æ˜¯æ‰€åœ¨æ–‡ä»¶å¤¹ã€æ–‡ä»¶åœ°å€ã€æ–‡ä»¶åœ°å€åˆ—è¡¨
    :param torch_dtype: bool, ç²¾åº¦, 'double', 'float', 'half', 'float16', 'bfloat16'
    :param quantization_config: dict, æ¨¡å‹é‡åŒ–ä½¿ç”¨åˆ°çš„å‚æ•°, eg. {'quant_method':'cpm_kernels', 'quantization_bit':8}
    :param generation_config: dict, genrerateä½¿ç”¨åˆ°çš„å‚æ•°, eg. {'mode':'random_sample', 'max_length':2048, 'default_rtype':'logits', 'use_states':True}
    :param create_model_at_startup: bool, æ˜¯å¦åœ¨å¯åŠ¨çš„æ—¶å€™åŠ è½½æ¨¡å‹, é»˜è®¤ä¸ºTrue
    :param system: Optional[str]=None, æ¨¡å‹ä½¿ç”¨çš„systemä¿¡æ¯, ä»…éƒ¨åˆ†æ¨¡å‹å¯ç”¨, ä¸”openai apiæ ¼å¼çš„ä¸éœ€è¦è®¾ç½®è¯¥å‚æ•°
"""

# ==========================================================================================
# =========================                 åŸºç±»                ============================
# ==========================================================================================
@add_start_docstrings(CHAT_START_DOCSTRING)
class ChatBase(PipeLineBase):
    def __init__(self, checkpoint_path:str, config_path:str=None, 
                 torch_dtype:Literal['double', 'float', 'half', 'float16', 'bfloat16', None]=None, 
                 quantization_config:dict=None, generation_config:dict=None, 
                 create_model_at_startup:bool=True, system:str=None, **kwargs):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.checkpoint_path = checkpoint_path
        self.config_path = config_path or checkpoint_path
        # generation_configé¡ºåºï¼šconfig -> æ˜¾å¼ä¼ å…¥generation_config -> kwargs
        config_path_tmp = get_config_path(self.config_path, allow_none=True)
        if config_path_tmp is not None:
            self.config = JsonConfig(config_path_tmp)
            self.generation_config = self.config.get('generation_config', dict())
        else:
            self.config = DottableDict()
            self.generation_config = dict()
        self.generation_config.update(generation_config if generation_config is not None else kwargs)
        self.torch_dtype = torch_dtype
        self.quantization_config = quantization_config
        kwargs['return_dict'] = False
        if create_model_at_startup:
            self.model = self.build_model(**kwargs)
        # tokenizeræ”¾åœ¨build_modelä¹‹åï¼Œé˜²æ­¢ç”¨æˆ·ä¼ å…¥çš„æ˜¯æ¨¡å‹åç§°éœ€è¦ä¸‹è½½
        self.tokenizer = self.build_tokenizer(**self.generation_config.get('tokenizer_config', dict()))
        self.generation_config['tokenizer'] = self.tokenizer
        self.template_config = self.config.get('template_config', dict())
        self.system = system
        self.kwargs = kwargs

    def no_history_states(self) -> bool:
        '''ä¸ä½¿ç”¨historyçš„states'''
        return self.generation_config.get('states') is None
    
    def build_prompt(self, *args, **kwargs) -> str:
        raise NotImplementedError
    
    def build_template(self, **kwargs):
        kwargs = {k:v for k,v in kwargs.items() if v}
        if self.template_config or kwargs:
            return Conversation(**{**self.template_config, **kwargs}).copy()
        return None
    
    def build_tokenizer(self, **kwargs):
        '''åˆå§‹åŒ–tokenizer'''
        from transformers import AutoTokenizer
        init_kwargs = {'additional_special_tokens'}
        new_kwargs = {k:v for k, v in kwargs.items() if k in init_kwargs}
        try:
            return AutoTokenizer.from_pretrained(self.checkpoint_path, trust_remote_code=True, **new_kwargs)
        except Exception as e:
            _, transformer_version = is_package_available('transformers', return_version=True)
            config_tmp = os.path.join(self.checkpoint_path, 'config.json')
            request_version = JsonConfig(config_tmp).get('transformers_version') if os.path.exists(config_tmp) else None
            if request_version is not None:
                log_error(f'Please check your transformer=={transformer_version}, while transformer=={request_version} requested.')
            else:
                log_error(f'Please check your transformer=={transformer_version}, which may not compatible.')
            raise e
        
    def process_response_history(self, response:Union[str,tuple,list], history:List[dict]=None) -> str:
        '''å¯¹responseå’Œhistryè¿›è¡Œåå¤„ç†
        1. å¯è‡ªè¡Œç»§æ‰¿åæ¥è‡ªå®šä¹‰
        2. historyæ˜¯æœ¬åœ°ä¿®æ”¹çš„, ç”¨äºå‘½ä»¤è¡Œæˆ–è€…web demoä¸‹ä¸€æ¬¡æ„å»ºå†å²ä½¿ç”¨çš„, responseå¯ä»¥ä¸ç­‰äºhistory[-1]['content']

        :param response: å¤§æ¨¡å‹ç›´æ¥è¾“å‡ºçš„å­—ç¬¦ä¸²
        :param history: èŠå¤©è®°å½•
            - role: è§’è‰² 
            - raw_content: æ¨¡å‹ç›´æ¥è¾“å‡ºçš„ç»“æœï¼Œå¯ç”¨äºcliæˆ–web demoçš„å±•ç¤º
            - content: å¤„ç†åçš„ï¼Œå¤šè½®å¯¹è¯ä¸­ç”¨äºpromptæ­å»º
            - function_call: functionè°ƒç”¨

        Returns: æ¥å£ç›´æ¥è¿”å›çš„å€¼ï¼ˆå¤„ç†åçš„response, è€Œä¸æ˜¯æ¨¡å‹ç›´æ¥è¾“å‡ºçš„ç»“æœï¼‰
        '''
        def process_history(res:str):
            res = res.strip()
            if history is None:
                return
            elif len(history) == 0:
                raise ValueError('history len can not be 0')
            elif history[-1]['role'] == 'user':
                history.append({"role": "assistant", "content": res, "raw_content": res})
            elif history[-1]['role'] == 'assistant':
                history[-1]["content"] = res
                history[-1]["raw_content"] = res

        if isinstance(response, str):
            process_history(response)
            return response
        elif isinstance(response, (tuple, list)):  # response, states
            assert len(response) == 2
            self.generation_config['states'] = response[1]
            process_history(response[0])
            return response[0]
        else:
            raise TypeError(f'`response` type={type(response)} which is not supported')

    @staticmethod
    def update_history(history:list, query:str):
        history.append({"role": "user", "content": query})
        return history
    
    def chat(self, query:Union[str, List[str]], history:List[dict]=None, functions:List[dict]=None, 
             return_history:bool=False, **kwargs) -> Union[str, List[str]]:
        '''chatæ¨¡å‹ä½¿ç”¨, é…åˆå¯¹è¯æ¨¡æ¿ä½¿ç”¨'''
        history = history or []

        if isinstance(query, str):
            # å•æ¡è¾“å…¥
            prompts:Union[str, torch.Tensor] = self.build_prompt(query, history, functions)
            response = self.model.generate(prompts, **self.generation_config)
            if isinstance(response, str):
                # ç”Ÿæˆå•æ¡è¾“å‡º
                response = self.process_response_history(response, history=history)
            elif isinstance(response, list):
                # ä¸ºå•æ¡queryç”Ÿæˆå¤šæ¡response
                response = [self.process_response_history(resp, history=copy.deepcopy(history)) for resp in response]
            else:
                raise TypeError(f'`response` type={type(response)} which is not supported')
            
        elif isinstance(query, list):
            # å¤šæ¡è¾“å…¥
            history_cp = [copy.deepcopy(history) for _ in query]
            prompts:List[str] = [self.build_prompt(q, h, functions) for q, h in zip(query, history_cp)]
            if all([isinstance(i, torch.Tensor) for i in prompts]):
                # build_promptè¿”å›çš„éƒ½æ˜¯tokenizeåçš„input_idsï¼Œéœ€è¦concat+paddingåœ¨ä¸€èµ·
                prompts = sequence_padding(prompts, value=self.tokenizer.pad_token_id, padding_side='left')
            response = self.model.generate(prompts, **self.generation_config)
            response = [self.process_response_history(r, history=h) for r, h in zip(response, history_cp)]
        else:
            raise TypeError(f'Args `query` type={type(query)} which is not supported')
        if return_history:
            return response, history
        else:
            return response
        
    def stream_chat(self, query:str, history:List[dict]=None, functions:List[dict]=None, **kwargs):
        '''chatæ¨¡å‹ä½¿ç”¨, é…åˆå¯¹è¯æ¨¡æ¿ä½¿ç”¨, å•æ¡æ ·æœ¬streamè¾“å‡ºé¢„æµ‹çš„ç»“æœ'''
        history = history or []
        prompt = self.build_prompt(query, history, functions)
        for response in self.model.stream_generate(prompt, **self.generation_config):
            yield self.process_response_history(response, history)

    def generate(self, query:Union[str, List[str]], **kwargs) -> Union[str, List[str]]:
        '''baseæ¨¡å‹ä½¿ç”¨'''
        return self.model.generate(query, **self.generation_config)

    def stream_generate(self, query:str):
        '''baseæ¨¡å‹ä½¿ç”¨, å•æ¡æ ·æœ¬streamè¾“å‡ºé¢„æµ‹çš„ç»“æœ'''
        yield from self.model.stream_generate(query, **self.generation_config)


# ==========================================================================================
# =========================              å‘½ä»¤è¡ŒèŠå¤©çš„åŸºç±»        ============================
# ==========================================================================================
@add_start_docstrings(CHAT_START_DOCSTRING)
class ChatCli(ChatBase):
    '''åœ¨å‘½ä»¤è¡Œä¸­äº¤äº’çš„demo
    :param init_str: str, å¯¹è¯é—®å€™å¥
    '''
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.init_str = kwargs.get('init_str', "è¾“å…¥å†…å®¹è¿›è¡Œå¯¹è¯ï¼Œclearæ¸…ç©ºå¯¹è¯å†å²ï¼Œstopç»ˆæ­¢ç¨‹åº")

    def build_cli_text(self, history:List[dict]) -> str:
        '''æ„å»ºå‘½ä»¤è¡Œç»ˆç«¯æ˜¾ç¤ºçš„text'''
        prompt = self.init_str
        for query_or_response in history:
            # ç°åœ¨çš„dictæ ¼å¼ï¼Œå½¢å¦‚{'role': 'user', 'content': 'ä½ å¥½å•Š'}
            if query_or_response['role'] == "user":
                prompt += f"\n\n{colorful('Userï¼š', color='green')}{query_or_response['content']}"
            elif query_or_response['role'] == "assistant":
                response = query_or_response.get('raw_content', query_or_response['content'])
                prompt += f"\n\n{colorful('Assistantï¼š', color='red')}{response}"
                # function_callä¸»è¦ç”¨äºcontentçš„ç»“æ„åŒ–å±•ç¤º
                if query_or_response.get('function_call'):
                    prompt += f"\n\n{colorful('Functionï¼š', color='yellow')}{query_or_response['function_call']}"
        return prompt

    def run(self, functions:List[dict]=None, stream:bool=True):
        import platform
        os_name = platform.system()
        history = []
        clear_command = 'cls' if os_name == 'Windows' else 'clear'
        print(self.init_str)
        while True:
            query = input(f"\n{colorful('Userï¼š', color='green')}")
            if query.strip() == "stop":
                break
            if query.strip() == "clear":
                history = []
                if 'states' in self.generation_config:
                    self.generation_config.pop('states')
                cuda_empty_cache()
                os.system(clear_command)
                print(self.init_str)
                continue
            
            prompt = self.build_prompt(query, history, functions)
            # historyæ˜¯humanå’Œassistantçš„èŠå¤©å†å²
            # æ ¼å¼å¦‚[{'role': 'user', 'content': 'ä½ å¥½'}, {'role': 'assistant', 'content': 'æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ'}]
            if stream:
                for response in self.model.stream_generate(prompt, **self.generation_config):
                    response = self.process_response_history(response, history)
                    os.system(clear_command)
                    print(self.build_cli_text(history), flush=True)
            else:
                response = self.model.generate(prompt, **self.generation_config)
                response = self.process_response_history(response, history)
                os.system(clear_command)
                print(self.build_cli_text(history), flush=True)
            cuda_empty_cache()


# ==========================================================================================
# =======================              ç½‘é¡µGradioèŠå¤©çš„åŸºç±»        ==========================
# ==========================================================================================
@add_start_docstrings(CHAT_START_DOCSTRING)
class ChatWebGradio(ChatBase):
    '''gradioå®ç°çš„ç½‘é¡µäº¤äº’çš„demo
    1. systemå’Œfunctionså‡åœ¨ç½‘é¡µä¸Šè¿›è¡Œè®¾ç½®(è‹¥æœ‰)
    '''
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        import gradio as gr
        self.gr = gr
        self.max_length = self.generation_config.get('max_length', 4096)
        self.max_repetition_penalty = kwargs.get('max_repetition_penalty', 10.0)
        self.max_temperature = kwargs.get('max_temperature', 10.0)
        if version.parse(gr.__version__) < version.parse("3.44.4"):
            log_warn_once('`gradio` changes frequently, the code is successfully tested under 3.44.4')

    def reset_user_input(self):
        return self.gr.update(value='')

    def reset_state(self):
        if 'states' in self.generation_config:
            self.generation_config.pop('states')
        cuda_empty_cache()  # æ¸…ç†æ˜¾å­˜
        return [], []

    def set_generation_config(self, max_length:int, top_p:float, temperature:float, repetition_penalty:float):
        '''æ ¹æ®webç•Œé¢çš„å‚æ•°ä¿®æ”¹ç”Ÿæˆå‚æ•°'''
        self.generation_config['max_length'] = max_length
        self.generation_config['top_p'] = top_p
        self.generation_config['temperature'] = temperature
        self.generation_config['repetition_penalty'] = repetition_penalty

    def _stream_predict(self, query, chatbot, history, max_length, top_p, temperature, repetition_penalty, system, functions):
        '''æµå¼ç”Ÿæˆ'''
        self.set_generation_config(max_length, top_p, temperature, repetition_penalty)
        chatbot.append((query, ""))
        functions = self._set_system_functions(system, functions)
        input_text = self.build_prompt(query, history, functions)
        for response in self.model.stream_generate(input_text, **self.generation_config):
            response = self.process_response_history(response, history)
            if history[-1].get('raw_content'):
                response = history[-1]['raw_content']
            if history[-1].get('function_call'):
                response += f"\n\nFunctionï¼š{history[-1]['function_call']}"
            chatbot[-1] = (query, response)
            yield chatbot, history
        cuda_empty_cache()  # æ¸…ç†æ˜¾å­˜

    def _set_system_functions(self, system:str=None, functions:List[dict]=None):
        '''è®¾ç½®systemå’Œfunctionså‚æ•°'''
        try:
            if functions is not None and functions.strip() != '':
                functions = json.loads(functions)
            else:
                functions = None
        except json.JSONDecodeError:
            functions = None
            log_warn('Functions implement not json format')
        if system.strip() != '':
            self.system = system
        return functions

    def regenerate(self, query, chatbot, history, max_length, top_p, temperature, repetition_penalty, system, functions):
        if chatbot and history and history[-1]['role'] == 'assistant':
            query, _ = chatbot.pop()
            history.pop()
            history.pop()
            yield from self._stream_predict(query, chatbot, history, max_length, top_p, temperature, repetition_penalty, system, functions)
        else:
            return chatbot, history
    
    def run(self, host:str=None, port:int=None, **launch_configs):
        with self.gr.Blocks() as demo:
            self.gr.HTML("""<h1 align="center">Chabot Gradio Demo</h1>""")

            with self.gr.Row():
                with self.gr.Column(scale=4):
                    chatbot = self.gr.Chatbot()
                    with self.gr.Column(scale=12):
                        query = self.gr.Textbox(show_label=False, placeholder="Input...", lines=10, max_lines=10) # .style(container=False)
                    with self.gr.Row():
                        submitBtn = self.gr.Button("ğŸš€ Submit", variant="primary")
                        regen_btn = self.gr.Button('ğŸ¤”ï¸ Regenerate')
                        emptyBtn = self.gr.Button("ğŸ§¹ Clear History")

                with self.gr.Column(scale=1):
                    max_length = self.gr.Slider(0, self.max_length, value=self.max_length, step=1.0, label="max_length", interactive=True)
                    top_p = self.gr.Slider(0, 1, value=self.generation_config.get('top_p', 1.0), step=0.01, label="top_p", interactive=True)
                    temperature = self.gr.Slider(0, self.max_temperature, value=self.generation_config.get('temperature', 1.0), step=0.1, label="temperature", interactive=True)
                    repetition_penalty = self.gr.Slider(0, self.max_repetition_penalty, value=self.generation_config.get('repetition_penalty', 1.0), step=0.1, label="repetition_penalty", interactive=True)
                    system = self.gr.Textbox(label='System Prompt (If exists)', lines=6, max_lines=6)
                    functions = self.gr.Textbox(label='Functions Json Format (If exists)', lines=6, max_lines=6)

            history = self.gr.State([])
            _input_tuple = [query, chatbot, history, max_length, top_p, temperature, repetition_penalty, system, functions]
            submitBtn.click(self._stream_predict, _input_tuple, [chatbot, history], show_progress=True)
            submitBtn.click(self.reset_user_input, [], [query])
            regen_btn.click(self.regenerate, _input_tuple, [chatbot, history], show_progress=True)
            emptyBtn.click(self.reset_state, outputs=[chatbot, history], show_progress=True)

        demo.queue().launch(server_name = launch_configs.pop('server_name', host), 
                            server_port = launch_configs.pop('server_port', port), 
                            **launch_configs)


# ==========================================================================================
# =====================              ç½‘é¡µstreamlitèŠå¤©çš„åŸºç±»        =========================
# ==========================================================================================
@add_start_docstrings(CHAT_START_DOCSTRING)
class ChatWebStreamlit(ChatBase):
    '''
    1. å¯åŠ¨æ–¹å¼: streamlit run app.py --server.address 0.0.0.0 --server.port 8001
    2. systemå’Œfunctionså‡åœ¨ç½‘é¡µä¸Šè¿›è¡Œè®¾ç½®(è‹¥æœ‰)
    '''
    def __init__(self, *args, **kwargs):
        if not is_streamlit_available():
            raise ModuleNotFoundError('pip install streamlit')
        if version.parse(st.__version__) < version.parse("1.29.0"):
            log_warn_once('`streamlit` is successfully tested under 1.29.0')
        st.set_page_config(
            page_title="Chabot Web Demo",
            page_icon=":robot:",
            layout="wide"
        )
        super().__init__(*args, **kwargs)
        log_warn_once('You should use command `streamlit run app.py --server.address 0.0.0.0 --server.port 8001` to launch')
        self.max_length = self.generation_config.get('max_length', 4096)
        self.max_repetition_penalty = kwargs.get('max_repetition_penalty', 10.0)
        self.max_temperature = kwargs.get('max_temperature', 10.0)

    @st.cache_resource
    def build_model(self, **kwarg):
        return super().build_model(**kwarg)
    
    @st.cache_resource
    def build_tokenizer(_self, **kwarg):
        return super().build_tokenizer(**kwarg)
    
    def run(self):
        if "history" not in st.session_state:
            st.session_state.history = []
        if "states" not in st.session_state:
            st.session_state.states = None

        max_length = st.sidebar.slider("max_length", 0, self.max_length, self.max_length//2, step=1)
        top_p = st.sidebar.slider("top_p", 0.0, 1.0, self.generation_config.get('top_p', 1.0), step=0.01)
        temperature = st.sidebar.slider("temperature", 0.0, self.max_temperature, self.generation_config.get('temperature', 1.0), step=0.01)
        repetition_penalty = st.sidebar.slider("repetition_penalty", 0.0, self.max_repetition_penalty, self.generation_config.get('repetition_penalty', 1.0), step=0.1)
        buttonClean = st.sidebar.button("Clear history", key="clean")
        if buttonClean:
            st.session_state.history = []
            st.session_state.states = None
            cuda_empty_cache()
            st.rerun()
        
        system = st.sidebar.text_area(
            label="System Prompt (If exists)",
            height=200,
            value="",
        )
        functions = st.sidebar.text_area(
            label="Functions Json Format (If exists)",
            height=200,
            value="",
        )

        try:
            if functions is not None and functions.strip() != '':
                functions = json.loads(functions)
            else:
                functions = None
        except json.JSONDecodeError:
            functions = None
            log_warn('Functions implement not json format')

        self.system = system

        for i, message in enumerate(st.session_state.history):
            role = message['role']
            if role not in {'user', 'assistant'}:
                continue
            with st.chat_message(name=role, avatar=role):
                st.markdown(message.get('raw_content', message['content']))
        
        with st.chat_message(name="user", avatar="user"):
            input_placeholder = st.empty()
        with st.chat_message(name="assistant", avatar="assistant"):
            message_placeholder = st.empty()

        query = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")
        if query:
            if query.strip() == "":
                st.warning('Input message could not be empty!', icon="âš ï¸")
            else:
                input_placeholder.markdown(query)
                history = st.session_state.history
                states = st.session_state.states
                self.generation_config['max_length'] = max_length
                self.generation_config['top_p'] = top_p
                self.generation_config['temperature'] = temperature
                self.generation_config['repetition_penalty'] = repetition_penalty
                self.generation_config['states'] = states

                input_text = self.build_prompt(query, history, functions)
                for response in self.model.stream_generate(input_text, **self.generation_config):
                    response = self.process_response_history(response, history)
                    message_placeholder.markdown(history[-1].get('raw_content', response))
                st.session_state.history = history
                st.session_state.states = self.generation_config.get('states')


# ==========================================================================================
# =========================              openai server          ============================
# ==========================================================================================
@asynccontextmanager
async def lifespan(app: FastAPI): # collects GPU memory
    yield
    cuda_empty_cache()


class ModelCard(BaseModel):
    id: str
    object: str = "model"
    created: int = Field(default_factory=lambda: int(time.time()))
    owned_by: str = "owner"
    root: Optional[str] = None
    parent: Optional[str] = None
    permission: Optional[list] = None


class ModelList(BaseModel):
    object: str = "list"
    data: List[ModelCard] = []


class ChatMessage(BaseModel):
    role: str
    content: Union[str, List[Dict]]
    function_call: Optional[Dict] = None


class DeltaMessage(BaseModel):
    role: Optional[str] = None
    content: Optional[str] = None
    function_call: Optional[Dict] = None


class ChatCompletionRequest(BaseModel):
    model: str = 'default'
    messages: List[ChatMessage]
    functions: Optional[List[Dict]] = None
    temperature: Optional[float] = None
    top_k: Optional[int] = None
    top_p: Optional[float] = None
    max_length: Optional[int] = None
    stream: Optional[bool] = False
    repetition_penalty: Optional[float] = None


class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: Literal['stop', 'length', 'function_call']


class ChatCompletionResponseStreamChoice(BaseModel):
    index: int
    delta: DeltaMessage
    finish_reason: Literal['stop', 'length', 'function_call', None]


class ChatCompletionResponse(BaseModel):
    model: str
    object: Literal["chat.completion", "chat.completion.chunk"]
    choices: List[Union[ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice]]
    created: Optional[int] = Field(default_factory=lambda: int(time.time()))


OPENAI_START_DOCSTRING = r"""
    éƒ¨ç½²ç±»ä¼¼OpenAiçš„api serverç«¯

    #
    :param checkpoint_path: str, æ¨¡å‹æ‰€åœ¨çš„æ–‡ä»¶å¤¹åœ°å€
    :param model_name: str, æ¨¡å‹åç§°
    :param generation_config: dict, æ¨¡å‹generateçš„å‚æ•°è®¾ç½®

    # openai apiå‚æ•°
    :param route_api: str, apiçš„è·¯ç”±
    :param route_models: str, æ¨¡å‹åˆ—è¡¨çš„è·¯ç”±
    :param offload_when_nocall: str, åœ¨ä¸€å®šæ—¶é•¿å†…æ— è°ƒç”¨å°±å¸è½½æ¨¡å‹ï¼Œå¯ä»¥å¸è½½åˆ°å†…å­˜å’Œdisk
    :param offload_max_callapi_interval: int, æœ€é•¿è°ƒç”¨é—´éš”
    :param offload_scheduler_interval: int, å®šæ—¶ä»»åŠ¡çš„æ‰§è¡Œé—´éš”
    :param api_keys: List[str], api keysçš„list
"""

@add_start_docstrings(OPENAI_START_DOCSTRING)
class ChatOpenaiApi(ChatBase):
    """
    TODO:
    1. åœ¨åç»­è°ƒç”¨æœåŠ¡ï¼Œæ¨¡å‹ä»cpuè½¬åˆ°cudaä¸Šæ—¶ï¼Œlinuxç¯å¢ƒä¸‹å†…å­˜ä¸‹é™ï¼Œæ˜¾å­˜ä¸èƒ½å®Œå…¨é™ä½ä¸º0
    2. ã€å·²ä¿®å¤ã€‘å¶ç„¶ä¼šå‘ç”Ÿè°ƒç”¨çš„æ—¶å€™ï¼Œä¸»çº¿ç¨‹å’Œå®šæ—¶çº¿ç¨‹æ‰“æ¶ï¼Œå¯¼è‡´deviceä¸ä¸€è‡´çš„é”™è¯¯ï¼Œå› ä¸ºforwardè¿‡ç¨‹æ—¶å€™å‘ç”Ÿoffload
    3. cpuå’Œdeleteæµ‹è¯•é€šè¿‡ï¼Œä½†æ˜¯å¦‚ä½•offloadåˆ°diskä¸Šï¼Œä¸å ç”¨å†…å­˜å’Œæ˜¾å­˜
    """
    def __init__(self, checkpoint_path:str, model_name:str='default', route_api:str='/chat/completions', route_models:str='/models', 
                 offload_max_callapi_interval:int=24*3600, offload_scheduler_interval:int=10*60, offload_when_nocall:Literal['cpu', 'disk', 'delete']=None, 
                 api_keys:List[str]=None, **kwargs):
        assert kwargs.get('system') is None, "Args `system` is used in request key `message`"
        super().__init__(checkpoint_path, **kwargs)
        if not is_fastapi_available():
            raise ModuleNotFoundError("No module found, use `pip install fastapi`")
        from sse_starlette.sse import EventSourceResponse
        import sse_starlette
        if version.parse(sse_starlette.__version__) > version.parse('1.8'):
            log_warn('Module `sse_starlette` above 1.8 not support stream output, use `pip install sse_starlette==1.6.5`')
        self.offload_when_nocall = offload_when_nocall
        if offload_max_callapi_interval <= offload_scheduler_interval:
            raise ValueError('Args `offload_scheduler_interval` must < `offload_max_callapi_interval`')
        self.offload_max_callapi_interval = offload_max_callapi_interval  # æœ€é•¿è°ƒç”¨é—´éš”
        self.offload_scheduler_interval = offload_scheduler_interval
        self.api_keys = api_keys
        self.EventSourceResponse = EventSourceResponse
        self.model_name = model_name
        self.role_user = 'user'
        self.role_assistant = 'assistant'

        if offload_when_nocall is None:
            self.app = FastAPI(lifespan=lifespan)
            self.lock = NoopContextManager()  # ä»…ç”¨äºå ä½ï¼Œè§„æ•´ä»£ç 
        else:
            # å¯ç”¨åå°ä»»åŠ¡ï¼Œç›‘æ§æ¥å£è°ƒç”¨æ¬¡æ•°
            self.app = FastAPI()
            self.app.add_event_handler("startup", self.startup_event)
            self.app.add_event_handler("shutdown", lambda: self.shutdown_event(self.app.state.scheduler))
            self.lock = threading.Lock()

        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )

        # æ·»åŠ è·¯ç”±
        router = APIRouter()
        router.add_api_route(route_models, methods=['GET'], endpoint=self.list_models, response_model=ModelList)
        router.add_api_route(route_api, methods=['POST'], endpoint=self.create_chat_completion, response_model=ChatCompletionResponse, 
                             dependencies=[Depends(self.check_api_key)])
        self.app.include_router(router)

    async def check_api_key(self, auth: Optional[HTTPAuthorizationCredentials] = Depends(HTTPBearer(auto_error=False))):
        if self.api_keys is not None:
            if auth is None or (token := auth.credentials) not in self.api_keys:
                raise HTTPException(
                    status_code=401,
                    detail={
                        "error": {
                            "message": "",
                            "type": "invalid_request_error",
                            "param": None,
                            "code": "invalid_api_key",
                        }
                    },
                )
            return token
        else:
            # api_keys not set; allow all
            return None
    
    def startup_event(self):
        from apscheduler.schedulers.background import BackgroundScheduler  
        scheduler = BackgroundScheduler()  
        scheduler.add_job(self.check_call_and_offload, 'interval', seconds=self.offload_scheduler_interval)
        scheduler.start()
        self.app.state.scheduler = scheduler  # å°†è°ƒåº¦å™¨å­˜å‚¨åœ¨appçš„çŠ¶æ€ä¸­ï¼Œä»¥ä¾¿åœ¨shutdownæ—¶ä½¿ç”¨  
    
    def shutdown_event(scheduler):  
        if scheduler:  
            scheduler.shutdown()

    async def list_models(self):
        model_card = ModelCard(id=self.model_name)
        return ModelList(data=[model_card])

    def prepare_build_prompt_args(self, request):
        '''å‡†å¤‡build_promptçš„å‚æ•°'''
        query = request.messages[-1].content
        history = [{'role': item.role, 'content': item.content} for item in request.messages[:-1]]
        input_args_or_kwargs = self.build_prompt(query, history, request.functions)
        return input_args_or_kwargs, history

    async def create_chat_completion(self, request: ChatCompletionRequest):
        if request.model != self.model_name:
            raise HTTPException(status_code=404, detail=f"Invalid model: request.model:{request.model} != self.model_name:{self.model_name}")
        if request.messages[-1].role != self.role_user:  # æœ€åä¸€æ¡msgçš„roleå¿…é¡»æ˜¯user
            raise HTTPException(status_code=400, detail=f"Invalid request: messages last role shold be {self.role_user}")

        if request.temperature:
            self.generation_config['temperature'] = request.temperature
        if request.top_p:
            self.generation_config['top_p'] = request.top_p
        if request.top_k:
            self.generation_config['top_k'] = request.top_k
        if request.max_length:
            self.generation_config['max_length'] = request.max_length
        if request.repetition_penalty:
            self.generation_config['repetition_penalty'] = request.repetition_penalty

        # å‡†å¤‡build_promptçš„å‚æ•°
        input_args_or_kwargs, history = self.prepare_build_prompt_args(request)

        with self.lock:
            self.model = self.build_model(**self.kwargs)
            self.last_callapi_timestamp = time.time()

            # æµå¼è¾“å‡º
            if request.stream:
                generate = self.predict(input_args_or_kwargs, request.model, history)
                return self.EventSourceResponse(generate, media_type="text/event-stream")
            
            # éæµå¼è¾“å‡º
            else:
                if isinstance(input_args_or_kwargs, (str,list)):
                    response = self.model.generate(input_args_or_kwargs, **self.generation_config)
                else:
                    response = self.model.generate(**input_args_or_kwargs, **self.generation_config)
                response = self.process_response_history(response, history)
                function_call = history[-1].get('function_call', None)
                choice_data = ChatCompletionResponseChoice(
                    index=0,
                    message=ChatMessage(role=self.role_assistant, content=response, function_call=function_call),
                    finish_reason= "function_call" if function_call is not None else "stop"
                )

                return ChatCompletionResponse(model=request.model, choices=[choice_data], object="chat.completion")

    async def predict(self, input_args_or_kwargs: str, model_id: str, history:list):
        choice_data = ChatCompletionResponseStreamChoice(
            index=0,
            delta=DeltaMessage(role=self.role_assistant),
            finish_reason=None
        )
        chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object="chat.completion.chunk")
        yield "{}".format(chunk.model_dump_json(exclude_unset=True))

        current_length = 0

        def get_generator(query):
            if isinstance(query, (str,list)):
                return self.model.stream_generate(query, **self.generation_config)
            else:
                return self.model.stream_generate(**query, **self.generation_config)
    
        for new_response in get_generator(input_args_or_kwargs):
            if len(new_response) == current_length:
                continue

            self.process_response_history(new_response, history)
            new_text = new_response[current_length:]
            current_length = len(new_response)

            function_call = history[-1].get('function_call', None)
            choice_data = ChatCompletionResponseStreamChoice(
                index=0,
                delta=DeltaMessage(content=new_text, function_call=function_call),
                finish_reason=None
            )
            chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object="chat.completion.chunk")
            yield "{}".format(chunk.model_dump_json(exclude_unset=True))

        function_call = history[-1].get('function_call', None)
        choice_data = ChatCompletionResponseStreamChoice(
            index=0,
            delta=DeltaMessage(function_call=function_call),
            finish_reason= "function_call" if function_call is not None else "stop"
        )
        chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object="chat.completion.chunk")
        yield "{}".format(chunk.model_dump_json(exclude_unset=True))
        yield '[DONE]'

    def check_call_and_offload(self):
        '''æ£€æµ‹è·ç¦»ä¸Šä¸€æ¬¡è°ƒç”¨è¶…å‡ºè§„å®šæ—¶é—´æ®µï¼Œè¶…å‡ºé—´éš”åˆ™offload'''
        now = time.time()
        if not hasattr(self, 'model')  or (self.model is None):
            return
        elif not hasattr(self, 'last_callapi_timestamp'):
            self.last_callapi_timestamp = now
        elif now - self.last_callapi_timestamp > self.offload_max_callapi_interval:  # è¶…å‡ºè°ƒç”¨é—´éš”
            cur = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time()))
            if (self.offload_when_nocall == 'cpu') and (str(self.model.device) != 'cpu'):
                with self.lock:
                    # å¦‚æœæ²¡æœ‰è°ƒç”¨ï¼Œå°†æ¨¡å‹è½¬ç§»åˆ°CPU
                    self.model.to('cpu')
                    log_free(f"{cur} - Model moved to cpu due to no activity for {self.offload_max_callapi_interval} sec.", prefix='[OFFLOAD]', prefix_color='cyan')
            elif (self.offload_when_nocall == 'disk') and hasattr(self, 'model'):
                with self.lock:
                    self.model = None
                    del self.model
                    log_free(f"{cur} - Model moved to disk due to no activity for {self.offload_max_callapi_interval} sec.", prefix='[OFFLOAD]', prefix_color='cyan')
            gc.collect()
            cuda_empty_cache()
        # print(now - self.last_callapi_timestamp)

    def run(self, app:str=None, host:str="0.0.0.0", port:int=8000, **kwargs):
        '''ä¸»ç¨‹åºå…¥å£'''
        import uvicorn
        uvicorn.run(app or self.app, host=host, port=port, **kwargs)


# ==========================================================================================
# =========================              å„ä¸ªå…·ä½“æ¨¡å‹å®ç°        ============================
# ==========================================================================================
@add_start_docstrings(CHAT_START_DOCSTRING)
class Glm(ChatBase):
    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        # æ²¡æœ‰systemå’Œfunction call
        if functions is not None:
            log_warn('ChatGlm do not support function call')
        
        if not history:
            prompt = query
        else:
            prompt, turn_i = "", 0
            if self.no_history_states():
                for query_or_response in history:
                    if query_or_response['role'] == 'user':
                        prompt += f"[Round {turn_i}]\né—®ï¼š{query_or_response['content']}\n"
                    elif query_or_response['role'] == 'assistant':
                        prompt += f"ç­”ï¼š{query_or_response['content']}\n"
                        turn_i += 1
            else:
                prompt += self.generation_config['states']['last_token']

            prompt += f"[Round {turn_i}]\né—®ï¼š{query}\nç­”ï¼š"
        history.append({'role': 'user', 'content': query})
        return prompt
    
    def process_response_history(self, response, history):
        response = response.strip()
        response = response.replace("[[è®­ç»ƒæ—¶é—´]]", "2023å¹´")
        punkts = [
            [",", "ï¼Œ"],
            ["!", "ï¼"],
            [":", "ï¼š"],
            [";", "ï¼›"],
            [r"\?", "ï¼Ÿ"],
        ]
        for item in punkts:
            response = re.sub(r"([\u4e00-\u9fff])%s" % item[0], r"\1%s" % item[1], response)
            response = re.sub(r"%s([\u4e00-\u9fff])" % item[0], r"%s\1" % item[1], response)
        response = super().process_response_history(response, history)
        return response


@add_start_docstrings(CHAT_START_DOCSTRING)
class Glm2(ChatBase):
    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        if functions is not None: 
            log_warn('ChatGlm2 do not support function call')

        # è¿™é‡Œå’Œchatglmçš„åŒºåˆ«æ˜¯ï¼Œchatglmçš„ç¬¬ä¸€è½®å¯¹è¯prompt=query, ä¸åŠ [Round 1]è¿™äº›å‰ç¼€
        prompt, turn_i = "", 1
        if self.no_history_states():
            for query_or_response in history:
                if query_or_response['role'] == 'user':
                    prompt += f"[Round {turn_i}]\n\né—®ï¼š{query_or_response['content']}\n\n"
                elif query_or_response['role'] == 'assistant':
                    prompt += f"ç­”ï¼š{query_or_response['content']}\n"
                    turn_i += 1
        else:
            prompt += self.generation_config['states']['last_token']

        prompt += f"[Round {turn_i}]\n\né—®ï¼š{query}\n\nç­”ï¼š"
        history.append({'role': 'user', 'content': query})
        return prompt
    
    def process_response_history(self, response:str, history:List[dict]):
        response = response.strip()
        response = response.replace("[[è®­ç»ƒæ—¶é—´]]", "2023å¹´")
        punkts = [
            [",", "ï¼Œ"],
            ["!", "ï¼"],
            [":", "ï¼š"],
            [";", "ï¼›"],
            [r"\?", "ï¼Ÿ"],
        ]
        for item in punkts:
            response = re.sub(r"([\u4e00-\u9fff])%s" % item[0], r"\1%s" % item[1], response)
            response = re.sub(r"%s([\u4e00-\u9fff])" % item[0], r"%s\1" % item[1], response)
        response = super().process_response_history(response, history)
        return response


@add_start_docstrings(CHAT_START_DOCSTRING)
class Glm3(ChatBase):
    ''' functionsæ ¼å¼å¦‚ä¸‹:
    ```python
    [
        {
            "name": "track", "description": "è¿½è¸ªæŒ‡å®šè‚¡ç¥¨çš„å®æ—¶ä»·æ ¼",
            "parameters":
                {
                    "type": "object", "properties":
                    {"symbol":
                        {
                            "description": "éœ€è¦è¿½è¸ªçš„è‚¡ç¥¨ä»£ç "
                        }
                    },
                    "required": []
                }
        }
    ]
    ```
    '''
    def __init__(self, *args, system:str=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.system = system

    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> list:
        if (len(history) == 0) or (history[0]["role"] != "system"):
            # å¢åŠ systemä¿¡æ¯
            if self.system:
                history.insert(0, {"role": "system", "content": self.system})
            elif functions is not None:
                history.insert(0, {
                    "role": "system", 
                    "content": "Answer the following questions as best as you can. You have access to the following tools:",
                    "tools": functions
                })

        # å¢åŠ toolsä¿¡æ¯
        if (functions is not None) and all(['tools' not in h for h in history]):
            history[0]['tools'] = functions

        if self.no_history_states():
            # ç”±äºtokenizerå°è£…äº†éƒ¨åˆ†é€»è¾‘ï¼Œè¿™é‡Œç›´æ¥è½¬æˆinput_ids
            input_ids = self.tokenizer.build_chat_input(query, history=history, role="user")['input_ids']
        else:
            input_ids += self.generation_config['states']['last_token']
        history = self.update_history(history, query)
        return input_ids
        
    def process_response_history(self, response:str, history:List[dict]):
        response = super().process_response_history(response, history)
        if (not response) or (response[-1] == "ï¿½"):
            return response

        content = ""
        for resp in response.split("<|assistant|>"):
            if "\n" in resp:
                metadata, content = resp.split("\n", maxsplit=1)
            else:
                metadata, content = "", resp
            
            metadata = metadata.strip()
            content = content.strip().replace("[[è®­ç»ƒæ—¶é—´]]", "2023å¹´")
            raw_content = resp.strip().replace("[[è®­ç»ƒæ—¶é—´]]", "2023å¹´")  # å¯ç”¨äºcliï¼Œweb_demoå±•ç¤º
            history[-1] = {"role": "assistant", "metadata": metadata, "content": content, "raw_content": raw_content}
            # æœ‰functions
            if metadata and history[0]["role"] == "system" and "tools" in history[0]:
                try:
                    # ä½¿ç”¨toolsæ—¶å€™ï¼Œstream_generateä¼šæœ‰é—®é¢˜ï¼Œå› ä¸ºä¸­é—´ç»“æœæ˜¯æ— æ³•ç»“æ„åŒ–è§£æçš„
                    def tool_call(**kwargs):
                        return kwargs
                    parameters = eval("\n".join(content.split("\n")[1:-1]))
                    history[-1]['function_call'] = {"name": metadata, "parameters": parameters}
                except SyntaxError:
                    pass
        return content


@add_start_docstrings(CHAT_START_DOCSTRING)
class Glm4(ChatBase):
    '''functionsæ ¼å¼å¦‚ä¸‹:
    ```python
    [
        {
            "type": "function",
            "function": {
                "name": "get_current_weather",
                "description": "Get the current weather",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "The temperature unit to use. Infer this from the users location.",
                        },
                    },
                    "required": ["location", "format"],
                },
            }
        },
    ]
    ```
    '''
    def __init__(self, *args, system:str=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.system = system

    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None):
        if (len(history) == 0) or (history[0]["role"] != "system"):
            # å¢åŠ systemä¿¡æ¯
            if self.system:
                history.insert(0, {"role": "system", "content": self.system})
            elif functions is not None:
                history.insert(0, {"role": "system", "tools": functions, "content": ""})

        # å¢åŠ toolsä¿¡æ¯
        if (functions is not None) and all(['tools' not in h for h in history]):
            history[0]['tools'] = functions

        # ç”±äºtokenizerå°è£…äº†éƒ¨åˆ†é€»è¾‘ï¼Œè¿™é‡Œç›´æ¥è½¬æˆinput_ids
        history = self.update_history(history, query)
        if self.no_history_states():
            prompt = self.tokenizer.apply_chat_template(history, add_generation_prompt=True, tokenize=False)
        else:
            prompt += self.generation_config['states']['last_token']
        return prompt
    
    def process_response_history(self, response:str, history:list):
        response = super().process_response_history(response, history)
        if (not response) or (response[-1] == "ï¿½"):
            return response

        content = ""
        for resp in response.split("<|assistant|>"):
            if "\n" in resp:
                metadata, content = resp.split("\n", maxsplit=1)
            else:
                metadata, content = "", resp
            
            metadata = metadata.strip()
            content = content.strip().replace("[[è®­ç»ƒæ—¶é—´]]", "2024å¹´")
            raw_content = resp.strip().replace("[[è®­ç»ƒæ—¶é—´]]", "2024å¹´")  # å¯ç”¨äºcliï¼Œweb_demoå±•ç¤º
            history[-1] = {"role": "assistant", "metadata": metadata, "content": content, "raw_content": raw_content}
            # æœ‰functions        
            if metadata and history[0]["role"] == "system" and "tools" in history[0]:
                history[-1]['function_call'] = {"name": metadata.strip(), "parameters": content}
        return content


@add_start_docstrings(CHAT_START_DOCSTRING)
class InternLM(ChatBase):
    def __init__(self, *args, system:str=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.system = system if system is not None else SYSTEM_ZH

    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None):
        # InternLM v1ä¸æ”¯æŒfunction call
        if functions is not None: 
            log_warn('InternLM do not support function call')
        if self.tokenizer.add_bos_token:
            prompt = ""
        else:
            prompt = self.tokenizer.bos_token
        
        if self.no_history_states():
            prompt += f"""<|System|>:{self.system}\n"""
            for query_or_response in history:
                if query_or_response['role'] == 'user':
                    prompt += f"""<|User|>:{query_or_response['content']}\n"""
                elif query_or_response['role'] == 'assistant':
                    prompt += f"""<|Bot|>:{query_or_response['content']}<eoa>\n"""
        else:
            prompt += self.generation_config['states']['last_token']

        prompt += f"""<|User|>:{query}\n<|Bot|>:"""
        history = self.update_history(history, query)
        return prompt

    def process_response_history(self, response, history=None):
        response = response.split("<eoa>")[0]
        response = super().process_response_history(response, history)
        return response


@add_start_docstrings(CHAT_START_DOCSTRING)
class InternLM2(ChatBase):
    '''internlm2æ”¯æŒfunction call, æ ¼å¼å¦‚ä¸‹:

    ç”±äº_additional_special_tokensä¸º['<|im_start|>', '<|im_end|>', '<|action_start|>', '<|action_end|>', '<|interpreter|>', '<|plugin|>']
    åœ¨function callæ—¶å€™è‹¥skip_special_tokens=True, åˆ™æ•æ‰ä¸åˆ°'<|action_start|>', '<|action_end|>', '<|interpreter|>', '<|plugin|>'
    å› æ­¤bert4torch_config.jsonä¸­æœªè®¾ç½®skip_special_tokens, é»˜è®¤ä¸ºFalse
    '''
    def __init__(self, *args, system:str=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.system = system if system is not None else SYSTEM_ZH
        self.plugin_with_name = True

        self.api_prefix = (
            "This is the subfunction for tool '{tool_name}', you can use this tool. "
            'The description of this function is: \n{description}')

        self.meta_prompt = ('å½“å¼€å¯å·¥å…·ä»¥åŠä»£ç æ—¶ï¼Œæ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·è¿›è¡Œè°ƒç”¨')

        INTERPRETER_CN = ('ä½ ç°åœ¨å·²ç»èƒ½å¤Ÿåœ¨ä¸€ä¸ªæœ‰çŠ¶æ€çš„ Jupyter ç¬”è®°æœ¬ç¯å¢ƒä¸­è¿è¡Œ Python ä»£ç ã€‚'
                        'å½“ä½ å‘ python å‘é€å«æœ‰ Python ä»£ç çš„æ¶ˆæ¯æ—¶ï¼Œå®ƒå°†åœ¨è¯¥ç¯å¢ƒä¸­æ‰§è¡Œã€‚'
                        'è¿™ä¸ªå·¥å…·é€‚ç”¨äºå¤šç§åœºæ™¯ï¼Œå¦‚æ•°æ®åˆ†ææˆ–å¤„ç†ï¼ˆåŒ…æ‹¬æ•°æ®æ“ä½œã€ç»Ÿè®¡åˆ†æã€å›¾è¡¨ç»˜åˆ¶ï¼‰ï¼Œ'
                        'å¤æ‚çš„è®¡ç®—é—®é¢˜ï¼ˆè§£å†³æ•°å­¦å’Œç‰©ç†éš¾é¢˜ï¼‰ï¼Œç¼–ç¨‹ç¤ºä¾‹ï¼ˆç†è§£ç¼–ç¨‹æ¦‚å¿µæˆ–ç‰¹æ€§ï¼‰ï¼Œ'
                        'æ–‡æœ¬å¤„ç†å’Œåˆ†æï¼ˆæ¯”å¦‚æ–‡æœ¬è§£æå’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰ï¼Œ'
                        'æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦ï¼ˆç”¨äºå±•ç¤ºæ¨¡å‹è®­ç»ƒå’Œæ•°æ®å¯è§†åŒ–ï¼‰ï¼Œ'
                        'ä»¥åŠæ–‡ä»¶æ“ä½œå’Œæ•°æ®å¯¼å…¥ï¼ˆå¤„ç†CSVã€JSONç­‰æ ¼å¼çš„æ–‡ä»¶ï¼‰ã€‚')

        self.plugin_prompt = ('ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹å·¥å…·ï¼š'
                    '\n{prompt}\n'
                    'å¦‚æœä½ å·²ç»è·å¾—è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·ç›´æ¥ç»™å‡ºç­”æ¡ˆ. é¿å…ä¸å¿…è¦çš„å·¥å…·è°ƒç”¨! '
                    'åŒæ—¶æ³¨æ„ä½ å¯ä»¥ä½¿ç”¨çš„å·¥å…·ï¼Œä¸è¦éšæ„æé€ ï¼')

    
    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None):
        if (len(history) == 0) or (history[0]["role"] != "system"):
            history.insert(0, {"role": "system", "content": self.system if functions is None else self.meta_prompt})

        if (functions is not None) and all([h['role'] !='function' for h in history]):
            # historyä¸­æ²¡æœ‰function
            start = [i for i, v in enumerate(history) if v['role']=='system'][-1] + 1
            plugin_descriptions = []
            for i, func in enumerate(functions, start=start):
                plugin = copy.deepcopy(func)
                name = plugin['name'].split('.')[0]
                plugin['description'] = self.api_prefix.format(tool_name=name, description=plugin['description'])
                plugin_descriptions.append(plugin)
            
            plugin_prompt = self.plugin_prompt.format(prompt=json.dumps(plugin_descriptions, ensure_ascii=False, indent=4))
            
            if self.plugin_with_name:
                content = f"""<|im_start|>system name=<|plugin|>\n{plugin_prompt}<|im_end|>\n"""
            else:
                content = f"""<|im_start|>system\n<|plugin|>\n{plugin_prompt}<|im_end|>\n"""
            history.insert(i, {"role": "function", "content": content})

        if self.tokenizer.add_bos_token:
            prompt = ""
        else:
            prompt = self.tokenizer.bos_token
        if self.no_history_states():
            for query_or_response in history:
                role, content = query_or_response['role'], query_or_response['content']
                if role == 'system':
                    prompt += f"""<|im_start|>system\n{content}<|im_end|>\n"""
                elif role == 'function':
                    prompt += content
                elif role == 'user':
                    prompt += f"""<|im_start|>user\n{content}<|im_end|>\n"""
                elif role == 'assistant':
                    prompt += f"""<|im_start|>assistant\n{content}<|im_end|>\n"""
        else:
            prompt += self.generation_config['states']['last_token']

        prompt += f"""<|im_start|>user\n{query}<|im_end|>\n<|im_start|>assistant\n"""
        history = self.update_history(history, query)
        return prompt

    def process_response_history(self, response, history=None):
        response = response.split("<|im_end|>")[0]
        for key in ['<|im_start|>', '<|im_end|>']:
            response = response.replace(key, '')
        response = super().process_response_history(response, history)

        start_token = '<|action_start|>'
        end_token = '<|action_end|>'
        for _token in ['<|plugin|>', '<|interpreter|>']:
            if _token in response:
                response, arguments = response.split(f"{start_token}{_token}")
                arguments = arguments.split(end_token)[0].strip()
                response = response.split(start_token)[0]
                history[-1]['function_call'] = {"name": 'plugin', "arguments": arguments}

        return response


@add_start_docstrings(CHAT_START_DOCSTRING)
class Qwen(ChatBase):
    '''functionsæ ¼å¼å¦‚ä¸‹:
    ```python
    [
        {
            'name_for_human': 'è°·æ­Œæœç´¢',
            'name_for_model': 'google_search',
            'description_for_model': 'è°·æ­Œæœç´¢æ˜¯ä¸€ä¸ªé€šç”¨æœç´¢å¼•æ“ï¼Œå¯ç”¨äºè®¿é—®äº’è”ç½‘ã€æŸ¥è¯¢ç™¾ç§‘çŸ¥è¯†ã€äº†è§£æ—¶äº‹æ–°é—»ç­‰ã€‚ Format the arguments as a JSON object.',
            'parameters': [{
                'name': 'search_query',
                'description': 'æœç´¢å…³é”®è¯æˆ–çŸ­è¯­',
                'required': True,
                'schema': {
                    'type': 'string'
                },
            }],
        },
    ]
    '''
    def __init__(self, *args, system:str=None, max_window_size=6144, **kwargs):
        super().__init__(*args, **kwargs)
        self.system = system if system is not None else 'You are a helpful assistant.'
        self.max_window_size = max_window_size
        self.observation_ids = self.tokenizer.encode('Observation:')

    def parse_messages(self, query, messages, functions):
        '''copy from https://github.com/QwenLM/Qwen/blob/main/openai_api.py'''

        TOOL_DESC = (
            '{name_for_model}: Call this tool to interact with the {name_for_human} API.'
            ' What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}'
        )

        REACT_INSTRUCTION = """Answer the following questions as best you can. You have access to the following APIs:

        {tools_text}

        Use the following format:

        Question: the input question you must answer
        Thought: you should always think about what to do
        Action: the action to take, should be one of [{tools_name_text}]
        Action Input: the input to the action
        Observation: the result of the action
        ... (this Thought/Action/Action Input/Observation can be repeated zero or more times)
        Thought: I now know the final answer
        Final Answer: the final answer to the original input question

        Begin!"""

        messages = copy.deepcopy(messages)
        if len(messages) > 0 and messages[0]['role'] == 'system':
            system = messages.pop(0)['content'].lstrip('\n').rstrip()
        else:
            system = self.system

        if functions:
            tools_text = []
            tools_name_text = []
            for func_info in functions:
                name = func_info.get('name', '')
                name_m = func_info.get('name_for_model', name)
                name_h = func_info.get('name_for_human', name)
                desc = func_info.get('description', '')
                desc_m = func_info.get('description_for_model', desc)
                tool = TOOL_DESC.format(
                    name_for_model=name_m,
                    name_for_human=name_h,
                    # Hint: You can add the following format requirements in description:
                    #   "Format the arguments as a JSON object."
                    #   "Enclose the code within triple backticks (`) at the beginning and end of the code."
                    description_for_model=desc_m,
                    parameters=json.dumps(func_info['parameters'], ensure_ascii=False),
                )
                tools_text.append(tool)
                tools_name_text.append(name_m)
            tools_text = '\n\n'.join(tools_text)
            tools_name_text = ', '.join(tools_name_text)
            instruction = (REACT_INSTRUCTION.format(
                tools_text=tools_text,
                tools_name_text=tools_name_text,
            ).lstrip('\n').rstrip())
        else:
            instruction = ''

        messages_with_fncall = messages
        messages = []
        for m_idx, m in enumerate(messages_with_fncall):
            role, content, func_call = m['role'], m['content'], m.get('function_call')
            content = content or ''
            content = content.lstrip('\n').rstrip()
            if role == 'function':
                if (len(messages) == 0) or (messages[-1]['role'] != 'assistant'):
                    raise ValueError('Expecting role assistant before role function.')
                messages[-1]['content'] += f'\nObservation: {content}'
                if m_idx == len(messages_with_fncall) - 1:
                    # add a prefix for text completion
                    messages[-1]['content'] += '\nThought:'
            elif role == 'assistant':
                if len(messages) == 0:
                    raise ValueError('Expecting role user before role assistant.')
                if func_call is None:
                    if functions:
                        content = f'Thought: I now know the final answer.\nFinal Answer: {content}'
                else:
                    f_name, f_args = func_call['name'], func_call['arguments']
                    if not content.startswith('Thought:'):
                        content = f'Thought: {content}'
                    content = f'{content}\nAction: {f_name}\nAction Input: {f_args}'
                if messages[-1]['role'] == 'user':
                    messages.append({'role': 'assistant', 'content': content.lstrip('\n').rstrip()})
                else:
                    messages[-1]['content'] += '\n' + content
            elif role == 'user':
                messages.append({'role': 'user', 'content': content.lstrip('\n').rstrip()})
            else:
                raise ValueError(f'Incorrect role {role}.')

        if len(messages) % 2 != 0:
            raise ValueError(f'{messages} len = {len(messages)}, not paired')

        history = []  # [(Q1, A1), (Q2, A2), ..., (Q_last_turn, A_last_turn)]
        for i in range(0, len(messages), 2):
            if messages[i]['role'] == 'user' and messages[i + 1]['role'] == 'assistant':
                usr_msg = messages[i]['content'].lstrip('\n').rstrip()
                bot_msg = messages[i + 1]['content'].lstrip('\n').rstrip()
                if instruction and (i == len(messages) - 2):
                    usr_msg = f'{instruction}\n\nQuestion: {usr_msg}'
                    instruction = ''
                history.append([usr_msg, bot_msg])
            else:
                raise ValueError('Expecting exactly one user (or function) role before every assistant role.')
        if instruction:
            query = f'{instruction}\n\nQuestion: {query}'
        return query, history, system

    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        if functions:
            # å¦‚æœä½¿ç”¨äº†functionsåˆ™éœ€è¦å¢åŠ eos_token_id
            if self.observation_ids not in self.generation_config.get('eos_token_id', []):
                self.generation_config['eos_token_id'] = self.generation_config.get('eos_token_id', []) + [self.observation_ids]

        instruction_query, history_list, system = self.parse_messages(query, history, functions)
        im_start, im_end = "<|im_start|>", "<|im_end|>"

        def _tokenize_str(role, content):
            return f"{role}\n{content}"

        system_text = _tokenize_str("system", system)
        raw_text = ""

        if self.no_history_states():
            for turn_query, turn_response in reversed(history_list):
                query_text = _tokenize_str("user", turn_query)
                response_text = _tokenize_str("assistant", turn_response)
                prev_chat = (
                    f"\n{im_start}{query_text}{im_end}\n{im_start}{response_text}{im_end}"
                )

                current_context_size = len(self.tokenizer.encode(raw_text, allowed_special={im_start, im_end}))
                if current_context_size < self.max_window_size:
                    raw_text = prev_chat + raw_text
                else:
                    break
            raw_text = f"{im_start}{system_text}{im_end}" + raw_text
        else:
            raw_text += self.generation_config['states']['last_token']

        raw_text += f"\n{im_start}user\n{instruction_query}{im_end}\n{im_start}assistant\n"
        history = self.update_history(history, query)
        return raw_text

    def process_response_history(self, response:Union[str,tuple,list], history:List[dict]=None) -> str:
        response = super().process_response_history(response, history)
        func_name, func_args = '', ''
        i = response.find('\nAction:')
        j = response.find('\nAction Input:')
        k = response.find('\nObservation:')
        if 0 <= i < j:  # If the text has `Action` and `Action input`,
            if k < j:  # but does not contain `Observation`,
                # then it is likely that `Observation` is omitted by the LLM,
                # because the output text may have discarded the stop word.
                response = response.rstrip() + '\nObservation:'  # Add it back.
            k = response.find('\nObservation:')
            func_name = response[i + len('\nAction:'):j].strip()
            func_args = response[j + len('\nAction Input:'):k].strip()

        if func_name:
            response = response[:i]
            t = response.find('Thought: ')
            if t >= 0:
                response = response[t + len('Thought: '):]
            response = response.strip()
            try:
                json.loads(func_args)
                history[-1]['function_call'] = {'name': func_name, 'arguments': func_args}
            except json.JSONDecodeError:
                pass
        else:
            z = response.rfind('\nFinal Answer: ')
            if z >= 0:
                response = response[z + len('\nFinal Answer: '):]
        history[-1]['content'] = response
        # if 'Observation:' in history[-1]['raw_content']:
        #     history[-1]['raw_content'] = history[-1]['raw_content'].replace('Observation:', '')
        return response


@add_start_docstrings(CHAT_START_DOCSTRING)
class Qwen2(ChatBase):
    '''Qwen2çš„chat, å«function callçš„é€»è¾‘
    ä¸»è¦å‚è€ƒäº†qwen_agentçš„é€»è¾‘
    :param parallel_function_calls: bool, å…è®¸å¹¶è¡Œè°ƒç”¨function toolså·¥å…·

    ### functionsæ ¼å¼å¦‚ä¸‹:
    ```python
    [
        {
            'name_for_human': 'æ–‡ç”Ÿå›¾',
            'name_for_model': 'image_gen',
            'description_for_model': 'æ–‡ç”Ÿå›¾æ˜¯ä¸€ä¸ªAIç»˜ç”»ï¼ˆå›¾åƒç”Ÿæˆï¼‰æœåŠ¡ï¼Œè¾“å…¥æ–‡æœ¬æè¿°ï¼Œè¿”å›æ ¹æ®æ–‡æœ¬ä½œç”»å¾—åˆ°çš„å›¾ç‰‡çš„URLã€‚ Format the arguments as a JSON object.',
            'parameters': [{
                'name': 'prompt',
                'description': 'è‹±æ–‡å…³é”®è¯ï¼Œæè¿°äº†å¸Œæœ›å›¾åƒå…·æœ‰ä»€ä¹ˆå†…å®¹',
                'required': True,
                'schema': {
                    'type': 'string'
                },
            }],
        },
    ]
    '''
    def __init__(self, *args, system:str=None, parallel_function_calls:bool=False, **kwargs):
        super().__init__(*args, **kwargs)
        self.system = system if system is not None else 'You are a helpful assistant.'
        self.parallel_function_calls = parallel_function_calls

        self.FN_NAME = 'âœ¿FUNCTIONâœ¿'
        self.FN_ARGS = 'âœ¿ARGSâœ¿'
        self.FN_RESULT = 'âœ¿RESULTâœ¿'
        self.FN_EXIT = 'âœ¿RETURNâœ¿'
        self.FN_STOP_WORDS = [self.FN_RESULT, self.FN_EXIT]
        self.FN_STOP_WORDS_IDS = [self.tokenizer.encode(i) for i in self.FN_STOP_WORDS]

        FN_CALL_TEMPLATE_INFO_ZH = """# å·¥å…·

        ## ä½ æ‹¥æœ‰å¦‚ä¸‹å·¥å…·ï¼š

        {tool_descs}"""

        FN_CALL_TEMPLATE_INFO_EN = """# Tools

        ## You have access to the following tools:

        {tool_descs}"""

        FN_CALL_TEMPLATE_FMT_ZH = """## ä½ å¯ä»¥åœ¨å›å¤ä¸­æ’å…¥é›¶æ¬¡ã€ä¸€æ¬¡æˆ–å¤šæ¬¡ä»¥ä¸‹å‘½ä»¤ä»¥è°ƒç”¨å·¥å…·ï¼š

        %s: å·¥å…·åç§°ï¼Œå¿…é¡»æ˜¯[{tool_names}]ä¹‹ä¸€ã€‚
        %s: å·¥å…·è¾“å…¥
        %s: å·¥å…·ç»“æœ
        %s: æ ¹æ®å·¥å…·ç»“æœè¿›è¡Œå›å¤ï¼Œéœ€å°†å›¾ç‰‡ç”¨![](url)æ¸²æŸ“å‡ºæ¥""" % (
            self.FN_NAME,
            self.FN_ARGS,
            self.FN_RESULT,
            self.FN_EXIT,
        )

        FN_CALL_TEMPLATE_FMT_EN = """## When you need to call a tool, please insert the following command in your reply, which can be called zero or multiple times according to your needs:

        %s: The tool to use, should be one of [{tool_names}]
        %s: The input of the tool
        %s: Tool results
        %s: Reply based on tool results. Images need to be rendered as ![](url)""" % (
            self.FN_NAME,
            self.FN_ARGS,
            self.FN_RESULT,
            self.FN_EXIT,
        )

        FN_CALL_TEMPLATE_FMT_PARA_ZH = """## ä½ å¯ä»¥åœ¨å›å¤ä¸­æ’å…¥ä»¥ä¸‹å‘½ä»¤ä»¥å¹¶è¡Œè°ƒç”¨Nä¸ªå·¥å…·ï¼š

        %s: å·¥å…·1çš„åç§°ï¼Œå¿…é¡»æ˜¯[{tool_names}]ä¹‹ä¸€
        %s: å·¥å…·1çš„è¾“å…¥
        %s: å·¥å…·2çš„åç§°
        %s: å·¥å…·2çš„è¾“å…¥
        ...
        %s: å·¥å…·Nçš„åç§°
        %s: å·¥å…·Nçš„è¾“å…¥
        %s: å·¥å…·1çš„ç»“æœ
        %s: å·¥å…·2çš„ç»“æœ
        ...
        %s: å·¥å…·Nçš„ç»“æœ
        %s: æ ¹æ®å·¥å…·ç»“æœè¿›è¡Œå›å¤ï¼Œéœ€å°†å›¾ç‰‡ç”¨![](url)æ¸²æŸ“å‡ºæ¥""" % (
            self.FN_NAME,
            self.FN_ARGS,
            self.FN_NAME,
            self.FN_ARGS,
            self.FN_NAME,
            self.FN_ARGS,
            self.FN_RESULT,
            self.FN_RESULT,
            self.FN_RESULT,
            self.FN_EXIT,
        )

        FN_CALL_TEMPLATE_FMT_PARA_EN = """## Insert the following command in your reply when you need to call N tools in parallel:

        %s: The name of tool 1, should be one of [{tool_names}]
        %s: The input of tool 1
        %s: The name of tool 2
        %s: The input of tool 2
        ...
        %s: The name of tool N
        %s: The input of tool N
        %s: The result of tool 1
        %s: The result of tool 2
        ...
        %s: The result of tool N
        %s: Reply based on tool results. Images need to be rendered as ![](url)""" % (
            self.FN_NAME,
            self.FN_ARGS,
            self.FN_NAME,
            self.FN_ARGS,
            self.FN_NAME,
            self.FN_ARGS,
            self.FN_RESULT,
            self.FN_RESULT,
            self.FN_RESULT,
            self.FN_EXIT,
        )

        self.FN_CALL_TEMPLATE = {
            'zh': FN_CALL_TEMPLATE_INFO_ZH + '\n\n' + FN_CALL_TEMPLATE_FMT_ZH,
            'en': FN_CALL_TEMPLATE_INFO_EN + '\n\n' + FN_CALL_TEMPLATE_FMT_EN,
            'zh_parallel': FN_CALL_TEMPLATE_INFO_ZH + '\n\n' + FN_CALL_TEMPLATE_FMT_PARA_ZH,
            'en_parallel': FN_CALL_TEMPLATE_INFO_EN + '\n\n' + FN_CALL_TEMPLATE_FMT_PARA_EN,
        }
        
    @staticmethod
    def get_function_description(function: Dict, lang: Literal['en', 'zh']) -> str:
        """
        Text description of function  # copy from qwen_agent
        """
        tool_desc_template = {
            'zh': '### {name_for_human}\n\n{name_for_model}: {description_for_model} è¾“å…¥å‚æ•°ï¼š{parameters} {args_format}',
            'en': '### {name_for_human}\n\n{name_for_model}: {description_for_model} Parameters: {parameters} {args_format}'
        }
        tool_desc = tool_desc_template[lang]
        name = function.get('name', None)
        name_for_human = function.get('name_for_human', name)
        name_for_model = function.get('name_for_model', name)
        assert name_for_human and name_for_model

        if name_for_model == 'code_interpreter':
            args_format = {
                'zh': 'æ­¤å·¥å…·çš„è¾“å…¥åº”ä¸ºMarkdownä»£ç å—ã€‚',
                'en': 'Enclose the code within triple backticks (`) at the beginning and end of the code.',
            }
        else:
            args_format = {
                'zh': 'æ­¤å·¥å…·çš„è¾“å…¥åº”ä¸ºJSONå¯¹è±¡ã€‚',
                'en': 'Format the arguments as a JSON object.',
            }
        args_format = function.get('args_format', args_format[lang])

        return tool_desc.format(name_for_human=name_for_human,
                                name_for_model=name_for_model,
                                description_for_model=function.get('description', function['description_for_model']),
                                parameters=json.dumps(function['parameters'], ensure_ascii=False),
                                args_format=args_format).rstrip()

    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        if functions:
            # å¦‚æœä½¿ç”¨äº†functionsåˆ™éœ€è¦å¢åŠ eos_token_id
            if all([i not in self.generation_config.get('eos_token_id', []) for i in self.FN_STOP_WORDS_IDS]):
                self.generation_config['eos_token_id'] = self.generation_config.get('eos_token_id', []) + self.FN_STOP_WORDS_IDS

        if (len(history) == 0) or (history[0]["role"] != "system"):
            history.insert(0, {"role": "system", "content": self.system})
        
        # å¤„ç†functionsçš„é€»è¾‘, copy from qwen_agent
        if (functions is not None) and all(['tools' not in h for h in history]):
            lang = 'en'
            for m in history:
                if has_chinese_char(m['content']):
                    lang = 'zh'
                    break

            tool_desc_template = self.FN_CALL_TEMPLATE[lang + ('_parallel' if self.parallel_function_calls else '')]
            tool_descs = '\n\n'.join(self.get_function_description(function, lang=lang) for function in functions)
            tool_names = ','.join(function.get('name', function.get('name_for_model', '')) for function in functions)
            tool_system = tool_desc_template.format(tool_descs=tool_descs, tool_names=tool_names)
            history[0]['content'] += '\n\n' + tool_system
            history[0]['tools'] = tool_system  # ä»…ç”¨äºæ˜¯å¦å·²ç»æ·»åŠ è¿‡functionsçš„åˆ¤æ–­

        history = self.update_history(history, query)
        if self.no_history_states():
            prompt = self.tokenizer.apply_chat_template(history, add_generation_prompt=True, tokenize=False)
        else:
            prompt += self.generation_config['states']['last_token']

        return prompt
    
    def process_response_history(self, response:Union[str,tuple,list], history:List[dict]=None) -> str:
        """
        If the model calls function by built-in function call template,
        convert and display it in function_call format.
        """
        response = super().process_response_history(response, history)

        # Remove ': ' brought by continued generation of function calling
        if response.startswith(': '):
            response = response[2:]
        elif response.startswith(':'):
            response = response[1:]

        i = response.find(f'{self.FN_NAME}:')

        # æ²¡æœ‰function call
        if i < 0:
            show_text = self.remove_incomplete_special_tokens(response)
            return show_text

        # åœ¨function callå‰è¯´äº†éƒ¨åˆ†æè¿°
        thought = None
        if i > 0:
            answer = response[:i].lstrip('\n').rstrip()
            if answer.endswith('\n'):
                answer = answer[:-1]
            thought = self.remove_incomplete_special_tokens(answer)
            # if thought:
            #     history[-1]['content'] = thought
            response = response[i:]

        # æœ‰function call
        for part in response.split(f'{self.FN_NAME}:'):
            if not part:
                continue
            if part.endswith('\n'):
                part = part[:-1]

            arg_sep = f'\n{self.FN_ARGS}:'
            i = part.find(arg_sep)
            if i < 0:
                fn_name = part.strip()
                list_of_fn_args = ['']
            else:
                fn_name = part[:i].strip()
                list_of_fn_args = [_.strip() for _ in part[i + len(arg_sep):].split(arg_sep)]
            fn_name = self.remove_incomplete_special_tokens(fn_name)
            for fn_args in list_of_fn_args:
                fn_args = self.remove_incomplete_special_tokens(fn_args)
                fn_args = self.remove_trailing_comment_of_fn_args(fn_args)
                history[-1]['function_call'] = {'name': fn_name, 'arguments': fn_args}
        return thought or response
    
    def remove_incomplete_special_tokens(self, text: str) -> str:
        special_tokens = (self.FN_NAME, self.FN_ARGS, self.FN_RESULT, self.FN_EXIT)
        text = text.rstrip()
        if text.endswith(special_tokens):
            for s in special_tokens:
                if text.endswith(s):
                    text = text[:-len(s)]
                    break
        else:
            trail_start = text.rfind('âœ¿')
            trail_token = text[trail_start:]
            for s in special_tokens:
                if s.startswith(trail_token):
                    text = text[:trail_start]
                    break
        text = text.lstrip('\n').rstrip()
        return text
    
    @staticmethod
    def remove_trailing_comment_of_fn_args(fn_args: str):
        fn_args = fn_args.strip()

        if fn_args.startswith('{'):
            k = fn_args.rfind('}')
            if k > 0:
                fn_args = fn_args[:k + 1]

        if fn_args.startswith('```'):
            k = fn_args.rfind('\n```')
            if k > 0:
                fn_args = fn_args[:k + 4]
        return fn_args


@add_start_docstrings(CHAT_START_DOCSTRING)
class LLaMA2(ChatBase):
    '''LLaMA2
    LLaMAç”±äºåªæœ‰baseæ¨¡å‹, æ²¡æœ‰chatæ‰€ä»¥ç›´æ¥model.generateå³å¯
    '''
    def __init__(self, *args, system:str=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.system = system

    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        if functions is not None: 
            log_warn('LLaMA2 do not support function call')

        if (len(history) == 0) or (history[0]["role"] != "system"):
            system = self.system or SYSTEM_ZH if has_chinese_char(query) else SYSTEM_EN
            history.insert(0, {"role": "system", "content": system})

        texts = ''
        if self.no_history_states():
            for query_or_response in history:
                role, content = query_or_response['role'], query_or_response['content'].strip()
                if role == 'system':
                    texts += f'[INST] <<SYS>>\n{content}\n<</SYS>>\n\n'
                elif role == 'user':
                    texts += f'{content} [/INST] '
                elif role == 'assistant':
                    texts += f"{content} </s><s> [INST] "
        else:
            texts = self.generation_config['states']['last_token']

        texts += f'{query.strip()} [/INST]'
        history = self.update_history(history, query)
        return texts


@add_start_docstrings(CHAT_START_DOCSTRING)
class ApplyChatTemplate(ChatBase):
    '''ç›´æ¥ä½¿ç”¨self.tokenizer.apply_chat_templateæ¥æ„å»ºè¾“å…¥
    å¦‚æœæ¨¡å‹ç›´æ¥æ²¿ç”¨è¿™ç§æ–¹å¼ï¼Œåˆ™æ— éœ€åšç‰¹æ®Šçš„å¤„ç†
    '''
    def __init__(self, *args, system:str=None, add_generation_prompt:bool=True, 
                 tokenize:bool=False, tools_in_user_message:bool=False, 
                 enable_thinking:bool=True, **kwargs):
        super().__init__(*args, **kwargs)
        self.system = system
        self.apply_chat_template_config = {
            "add_generation_prompt": add_generation_prompt, 
            "tokenize": tokenize,
            "tools_in_user_message": tools_in_user_message,
            "enable_thinking": enable_thinking
        }

    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        if self.system and ((len(history) == 0) or (history[0]["role"] != "system")):
            history.insert(0, {"role": "system", "content": self.system})

        history = self.update_history(history, query)
        if self.no_history_states():
            # llama3.1æ”¯æŒfunction call
            tools = functions
            if (functions is not None) and (not isinstance(functions, list)):
                tools = [functions]
            texts = self.tokenizer.apply_chat_template(history, tools = tools, **self.apply_chat_template_config)
        else:
            texts = self.generation_config['states']['last_token']
            texts += f'<|start_header_id|>user<|end_header_id|>\n\n{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n'
        return texts
    
    def process_response_history(self, response:Union[str,tuple,list], history:List[dict]=None) -> str:
        response = super().process_response_history(response, history)
        try:
            json.loads(response)
            history[-1]['function_call'] = response
        except json.JSONDecodeError:
            pass
        return response

@add_start_docstrings(CHAT_START_DOCSTRING)
class LLaMA3(ApplyChatTemplate):
    '''llama3ä¸æ”¯æŒfunction call, llama3.1æ”¯æŒfunction call
    
    ### LLaMA3.1è¯·æ±‚çš„Example
    ```json
    [
        {"role": "system", "content": "You are a bot that responds to weather queries."},
        {"role": "user", "content": "Hey, what's the temperature in Paris right now?"},
        {"role": "assistant", "tool_calls": [{"type": "function", "function": tool_call}]},
        {"role": "tool", "name": "get_current_temperature", "content": "22.0"}
    ]
    ```
    '''
    pass


@add_start_docstrings(CHAT_START_DOCSTRING)
class DeepSeekR1(ApplyChatTemplate):
    '''ç›´æ¥ä½¿ç”¨self.tokenizer.apply_chat_templateæ¥æ„å»ºè¾“å…¥
    å¦‚æœæ¨¡å‹ç›´æ¥æ²¿ç”¨è¿™ç§æ–¹å¼ï¼Œåˆ™æ— éœ€åšç‰¹æ®Šçš„å¤„ç†
    '''
    def process_response_history(self, response:Union[str,tuple,list], history:List[dict]=None) -> str:
        response = super().process_response_history(response, history)
        if '</think>' in response:
            history[-1]['content'] = response.split('</think>')[-1].strip()
        return response


@add_start_docstrings(CHAT_START_DOCSTRING)
class Ziya(ChatBase):
    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        if functions is not None: 
            log_warn('Ziya do not support function call')

        prompt = ''
        if self.no_history_states():
            for query_or_response in history:
                role, content = query_or_response['role'], query_or_response['content']
                if role == 'user':
                    prompt += f"<human>:{content}\n"
                elif role == 'assistant':
                    prompt += f"<bot>:{content}\n"
        else:
            prompt += self.generation_config['states']['last_token']
        
        prompt += f"<human>:{query.strip()}\n<bot>:"
        history = self.update_history(history, query)
        return prompt


@add_start_docstrings(CHAT_START_DOCSTRING)
class ChineseLlamaAlpaca(ChatBase):
    def __init__(self, *args, system:str=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.system = system or \
("Below is an instruction that describes a task. "
"Write a response that appropriately completes the request.\n\n"
)

    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        if functions is not None: 
            log_warn('ChineseAlphaLLaMA do not support function call')

        if (len(history) == 0) or (history[0]["role"] != "system"):
            history.insert(0, {"role": "system", "content": self.system})

        prompt = ''
        if self.no_history_states():
            for query_or_response in history:
                role, content = query_or_response['role'], query_or_response['content']
                if role == 'system':
                    prompt = self.system + prompt
                elif role == 'user':
                    prompt += f"### Instruction:\n\n{content}\n\n"
                elif role == 'assistant':
                    prompt += f"### Response:\n\n{content}\n\n"
            prompt += f"### Instruction:\n\n{query}\n\n### Response:\n\n"
        else:
            prompt += self.generation_config['states']['last_token'] + f"### Instruction:\n\n{query}\n\n### Response:\n\n"
        
        history = self.update_history(history, query)
        return prompt


@add_start_docstrings(CHAT_START_DOCSTRING)
class Belle(ChatBase):
    def build_tokenizer(self, **kwargs):
        from transformers import AutoTokenizer
        return AutoTokenizer.from_pretrained(self.checkpoint_path, use_fast=False)
    
    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        if functions is not None: 
            log_warn('Belle do not support function call')

        prompt = ''
        if self.no_history_states():
            for query_or_response in history:
                role, content = query_or_response['role'], query_or_response['content']
                if role == 'user':
                    prompt += f"Human: {content} \n\n"
                elif role == 'assistant':
                    prompt += f"Assistant: {content}\n\n"
        else:
            prompt += self.generation_config['states']['last_token']
        prompt += f"Human: {query} \n\nAssistant: "
        history = self.update_history(history, query)
        return prompt


@add_start_docstrings(CHAT_START_DOCSTRING)
class Baichuan(ChatBase):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.user_token_id = kwargs.get('user_token_id', 195)
        self.assistant_token_id = kwargs.get('assistant_token_id', 196)

    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        if functions is not None: 
            log_warn('Baichuan do not support function call')

        total_input = []
        if self.no_history_states():
            for query_or_response in history:
                role, content = query_or_response['role'], query_or_response['content']
                if role == 'user':
                    total_input += [self.user_token_id] + self.tokenizer.encode(content)
                elif role == 'assistant':
                    total_input += [self.assistant_token_id] + self.tokenizer.encode(content) + [self.tokenizer.eos_token_id]
        else:
            total_input += [self.generation_config['states']['last_token_id']]
        total_input += [self.user_token_id] + self.tokenizer.encode(query) + [self.assistant_token_id]
        
        history = self.update_history(history, query)
        return total_input


@add_start_docstrings(CHAT_START_DOCSTRING)
class PretrainedTextContinuation(ChatBase):
    '''é¢„è®­ç»ƒçš„æ¨¡å‹ç»­å†™'''
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def build_prompt(self, query:str, history:List[dict], functions:List[dict]=None) -> str:
        if functions is not None: 
            log_warn('PretrainedTextContinuation do not support function call')

        total_input = ''
        if self.no_history_states():
            for query_or_response in history:
                role, content = query_or_response['role'], query_or_response['content']
                if self.generation_config.get('include_input', False):
                    if role == 'assistant':
                        total_input += content
                else:
                    total_input += content
        else:
            total_input += [self.generation_config['states']['last_token_id']]
        total_input += query
        
        history = self.update_history(history, query)
        return total_input


LLM_MAPPING = {
    'glm': Glm,
    'glm2': Glm2,
    'glm3': Glm3,
    'glm4': Glm4,
    'internlm': InternLM,
    'internlm2': InternLM2,
    'qwen': Qwen,
    'qwen2': Qwen2,
    'qwen3': ApplyChatTemplate,
    'qwen3_moe': ApplyChatTemplate,
    'llama2': LLaMA2,
    'llama3': LLaMA3,
    'ziya': Ziya,
    'chinese_llama_alpaca': ChineseLlamaAlpaca,
    'belle': Belle,
    'baichuan': Baichuan,
    'apply_chat_template': ApplyChatTemplate,
    'pretrained_text_continuation': PretrainedTextContinuation,
    'deepseek_r1': DeepSeekR1
}
