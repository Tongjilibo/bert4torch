''' å¤§æ¨¡å‹èŠå¤©çš„pipelineè°ƒç”¨
ä¸»è¦åŠŸèƒ½ï¼š
1. å‘½ä»¤è¡Œè°ƒç”¨å„ä¸ªæ¨¡å‹demo
2. åˆ©ç”¨fastapiä¸ºå¤§æ¨¡å‹æ­å»ºopenaiæ ¼å¼çš„serverå’Œclientè°ƒç”¨
    Implements API for LLM in OpenAI's format. (https://platform.openai.com/docs/api-reference/chat)
    Usage: python openai_api.py
    Visit http://localhost:8000/docs for documents.
3. webç•Œé¢å¿«é€Ÿæ­å»ºdemo(gradio+streamlit)

# TODO: è®¾ç½®return_states=Trueæ—¶å€™ï¼Œå—åˆ°build_promptå½±å“ï¼Œå¾ˆéš¾ä¿è¯promptå®Œå…¨å¤ç°
è¿™é‡Œé‡‡ç”¨æ·»åŠ self.generation_config['states']['last_token']ï¼Œæ˜¯å› ä¸ºæ¨ç†å®Œæˆå¯èƒ½æ˜¯å› ä¸ºåˆ°è¾¾max_lengthï¼Œæœªå¿…æ˜¯é‡åˆ°äº†eos
'''

import torch
from typing import Union, Optional, List, Tuple, Literal, Dict
from bert4torch.pipelines.chat import ChatBase, ChatWebGradio, ChatOpenaiApi, CHAT_START_DOCSTRING, OPENAI_START_DOCSTRING
from bert4torch.snippets import (
    log_warn_once, 
    get_config_path, 
    log_info, 
    log_info_once,
    log_warn, 
    log_error,
    cuda_empty_cache,
    is_fastapi_available, 
    is_pydantic_available, 
    is_sseclient_available, 
    is_streamlit_available,
    is_package_available,
    has_chinese_char,
    add_start_docstrings,
    JsonConfig,
    is_transformers_available,
    inference_mode
)
from packaging import version
import gc
import time
import json
import requests
from contextlib import asynccontextmanager
import threading
import re
import copy
from argparse import REMAINDER, ArgumentParser
from copy import deepcopy
from PIL import Image
import inspect
import numpy as np
import os


if is_fastapi_available():
    from fastapi import FastAPI, HTTPException, APIRouter, Depends
    from fastapi.security.http import HTTPAuthorizationCredentials, HTTPBearer
    from fastapi.middleware.cors import CORSMiddleware
else:
    class FastAPI: pass
    class HTTPAuthorizationCredentials: pass
    Depends, HTTPBearer = object, object

if is_pydantic_available():
    from pydantic import BaseModel, Field
else:
    BaseModel, Field = object, object

if is_streamlit_available():
    import streamlit as st
else:
    # é˜²æ­¢streamlitä¸å­˜åœ¨æ—¶å€™æŠ¥é”™
    import bert4torch.snippets as st
    st.cache_resource = st.delete_arguments

if is_transformers_available():
    from transformers import AutoProcessor

__all__ = [
    'ChatVBase',
    'MiniCPMV',
    "ChatV"
    ]


class ChatVBase(ChatBase):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.processor = AutoProcessor.from_pretrained(self.checkpoint_path, trust_remote_code=True)

    def generate(self, *args, **kwargs):
        '''baseæ¨¡å‹ä½¿ç”¨'''
        return self.model.generate(*args, **kwargs)

    def build_prompt(self, *args, **kwargs) -> str:
        raise NotImplementedError

    @inference_mode()
    def chat(
        self,
        query,
        images,
        history:List[dict]=None,
        vision_hidden_states=None,
        max_inp_length=8192,
        system_prompt='',
        max_slice_nums=None,
        use_image_id=None,
        **kwargs
    ):
        # å¤„ç†queryå’Œimagesè¾“å…¥
        inputs = self.build_prompt(query, images, history, max_inp_length=max_inp_length, max_slice_nums=max_slice_nums,
                                   system_prompt=system_prompt, use_image_id=use_image_id)
        answer = self.generate(
            **inputs,
            vision_hidden_states=vision_hidden_states,
            **self.generation_config,
            **kwargs
        )
        return answer


class ChatVWebGradio(ChatWebGradio):
    '''éœ€è¦æ·»åŠ ä¸€ä¸ªå›¾ç‰‡çš„ä¸Šä¼ '''
    @staticmethod
    def get_image_vedio(chatbot):
        def _is_video_file(filename):
            video_extensions = ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.mpeg']
            return any(filename.lower().endswith(ext) for ext in video_extensions)
        
        input_image, input_vedio = None, None
        if chatbot and os.path.isfile(chatbot[-1][0]):
            if _is_video_file(chatbot[-1][0]):
                # è§†é¢‘
                input_vedio = chatbot[-1][0]
            else:
                # å›¾ç‰‡
                input_image = chatbot[-1][0]
        return input_image, input_image
            

    def __stream_predict(self, query, chatbot, history, max_length, top_p, temperature, repetition_penalty, system, functions):
        '''æµå¼ç”Ÿæˆ'''
        self.set_generation_config(max_length, top_p, temperature, repetition_penalty)
        input_image, input_vedio = self.get_image_vedio(chatbot)
        chatbot.append((query, ""))
        functions = self._set_system_functions(system, functions)
        input_kwargs = self.build_prompt(query, input_image, input_vedio, history, functions)
        for response in self.model.stream_generate(**input_kwargs, **self.generation_config):
            response = self.process_response_history(response, history)
            if history[-1].get('raw_content'):
                response = history[-1]['raw_content']
            if history[-1].get('function_call'):
                response += f"\n\nFunctionï¼š{history[-1]['function_call']}"
            chatbot[-1] = (query, response)
            yield chatbot, history
        cuda_empty_cache()  # æ¸…ç†æ˜¾å­˜

    def run(self, host:str=None, port:int=None, **launch_configs):

        def add_file(chatbot, file):
            chatbot = chatbot if chatbot is not None else []
            chatbot = chatbot + [((file.name,), None)]
            return chatbot

        with self.gr.Blocks() as demo:
            self.gr.HTML("""<h1 align="center">Chabot Gradio Demo</h1>""")
            with self.gr.Row():
                with self.gr.Column(scale=4):
                    chatbot = self.gr.Chatbot(height=600)
                    with self.gr.Column(scale=12):
                        query = self.gr.Textbox(show_label=False, placeholder="Input...", lines=10, max_lines=10) # .style(container=False)
                    with self.gr.Row():
                        addfile_btn = self.gr.UploadButton('ğŸ“ Upload', file_types=['image', 'video'])
                        submitBtn = self.gr.Button("ğŸš€ Submit", variant="primary")
                        regen_btn = self.gr.Button('ğŸ¤”ï¸ Regenerate')
                        emptyBtn = self.gr.Button("ğŸ§¹ Clear History")

                with self.gr.Column(scale=1):
                    max_length = self.gr.Slider(0, self.max_length, value=self.max_length, step=1.0, label="max_length", interactive=True)
                    top_p = self.gr.Slider(0, 1, value=1.0, step=0.01, label="top_p", interactive=True)
                    temperature = self.gr.Slider(0, self.max_temperature, value=1.0, step=0.1, label="temperature", interactive=True)
                    repetition_penalty = self.gr.Slider(0, self.max_repetition_penalty, value=1.0, step=0.1, label="repetition_penalty", interactive=True)
                    system = self.gr.Textbox(label='System Prompt (If exists)', lines=6, max_lines=6)
                    functions = self.gr.Textbox(label='Functions Json Format (If exists)', lines=6, max_lines=6)
                
            history = self.gr.State([])
            _input_tuple = [query, chatbot, history, max_length, top_p, temperature, repetition_penalty, system, functions]
            addfile_btn.upload(add_file, [chatbot, addfile_btn], [chatbot], show_progress=True)
            submitBtn.click(self.__stream_predict, _input_tuple, [chatbot, history], show_progress=True)
            submitBtn.click(self.reset_user_input, [], [query])
            # regen_btn.click(regenerate, [chatbot], [chatbot], show_progress=True)
            emptyBtn.click(self.reset_state, outputs=[chatbot, history], show_progress=True)

        demo.queue().launch(server_name = launch_configs.pop('server_name', host), 
                            server_port = launch_configs.pop('server_port', port), 
                            **launch_configs)


class ChatVWebStreamlit(ChatWebGradio):
    pass


class ChatVOpenaiApi(ChatOpenaiApi):
    pass

ImageType = Union[str, Image.Image, np.ndarray]
def trans_images(images:Union[ImageType, List[ImageType], List[List[ImageType]]]):
    '''æŠŠå„ç§ç±»å‹çš„imagesè½¬åŒ–ä¸ºImage.Imageæ ¼å¼'''
    if isinstance(images, str):
        images = Image.open(images).convert('RGB')
    elif isinstance(images, np.ndarray):
        images = Image.fromarray(images)
    elif isinstance(images, List) and all([isinstance(image, (str, Image.Image, np.ndarray)) for image in images]):
        images = [trans_images(image) for image in images]
    elif isinstance(images, List) and all([isinstance(image, List) for image in images]):
        images = [trans_images(image) for image in images]
    return images

class MiniCPMV(ChatVBase):
    def build_prompt(
            self,
            queries: Union[str, List[str]], 
            images: Union[Image.Image, List[Image.Image], List[List[Image.Image]]], 
            vedios=None,
            history: List[Dict]=None, 
            functions=None,
            **kwargs
        ) -> str:
        '''
        history: [
                    {'role': 'user', 'content': 'å›¾ç‰‡ä¸­æè¿°çš„æ˜¯ä»€ä¹ˆ', 'images': [PIL.Image.Image]},
                    {'role': 'assistant', 'content': 'è¯¥å›¾ç‰‡ä¸­æè¿°äº†ä¸€ä¸ªå°ç”·å­©åœ¨è¸¢è¶³çƒ'},
                 ]

        |    query    |        images      |     comment      |
        |   -------   |      --------      |    ---------     |
        |     str     |        Image       |    æé—®å•å¼ å›¾ç‰‡   |
        |     str     |     List[Image]    |  åŒæ—¶æé—®å¤šå¼ å›¾ç‰‡  |
        |  List[str]  |        Image       |  å¤šæ¬¡æé—®å•å¼ å›¾ç‰‡  |
        |  List[str]  |     List[Image]    |  å„è‡ªæé—®å•å¼ å›¾ç‰‡  |
        |  List[str]  |  List[List[Image]] |å„è‡ªåŒæ—¶æé—®å¤šå¼ å›¾ç‰‡|
        '''
        images = trans_images(images)
        if isinstance(queries, str):
            queries = [queries]
            if isinstance(images, Image.Image):
                # æé—®å•å¼ å›¾ç‰‡
                images = [images]
            elif isinstance(images, List) and isinstance(images[0], Image.Image):
                # åŒæ—¶æé—®å¤šå¼ å›¾ç‰‡
                images = [images]
            elif images is None:
                images = [images]
        elif isinstance(queries, List) and isinstance(queries[0], str):
            if isinstance(images, Image.Image):
                # å¤šæ¬¡æé—®å•å¼ å›¾ç‰‡
                images = [images] * len(queries)
            elif isinstance(images, List) and isinstance(images[0], Image.Image):
                # å„è‡ªæé—®å•å¼ å›¾ç‰‡
                pass
            elif isinstance(images, List) and isinstance(images[0], List) and isinstance(images[0][0], Image.Image):
                # å„è‡ªåŒæ—¶æé—®å¤šå¼ å›¾ç‰‡
                pass

        assert len(queries) == len(images), "The batch dim of query and images should be the same."        
        assert self.model.config.query_num == self.processor.image_processor.image_feature_size, "These two values should be the same. Check `config.json` and `preprocessor_config.json`."
        assert self.model.config.patch_size == self.processor.image_processor.patch_size, "These two values should be the same. Check `config.json` and `preprocessor_config.json`."
        # assert self.model.config.use_image_id == self.processor.image_processor.use_image_id, "These two values should be the same. Check `config.json` and `preprocessor_config.json`."
        assert self.model.config.slice_config.max_slice_nums == self.processor.image_processor.max_slice_nums, "These two values should be the same. Check `config.json` and `preprocessor_config.json`."
        # assert self.model.config.slice_mode == self.processor.image_processor.slice_mode, "These two values should be the same. Check `config.json` and `preprocessor_config.json`."

        # å¤„ç†history
        history_images = []
        history_copy = copy.deepcopy(history) or []
        for i, hist in enumerate(history_copy):
            role = hist["role"]
            content = hist["content"]
            assert role in ["user", "assistant"]
            if i == 0:
                assert role == "user", "The role of first msg should be user"
            
            if 'images' not in hist:
                continue
            if isinstance(hist["images"], Image.Image):
                hist["images"] = [hist["images"]]
            hist["content"] = "(<image>./</image>)\n"*len(hist["images"]) + content
            history_images.extend(hist["images"])

        prompts_lists = []
        input_images_lists = []
        for query, image in zip(queries, images):
            copy_msgs = copy.deepcopy(history_copy) or []
            if image is None:
                image = []
            elif isinstance(image, Image.Image):
                image = [image]
            content = "(<image>./</image>)\n"*len(image) + query
            copy_msgs.append({'role': 'user', 'content': content})

            if kwargs.get('system_prompt'):
                sys_msg = {'role': 'system', 'content': kwargs.get('system_prompt')}
                copy_msgs = [sys_msg] + copy_msgs        

            prompts_lists.append(self.processor.tokenizer.apply_chat_template(copy_msgs, tokenize=False, add_generation_prompt=True))
            input_images_lists.append(history_images + image)
        
        if 'max_slice_nums' in inspect.signature(self.processor).parameters:
            # MiniCPM-V-2_6
            inputs = self.processor(
                prompts_lists, 
                input_images_lists, 
                max_slice_nums=kwargs.get('max_slice_nums'),
                use_image_id=kwargs.get('use_image_id'),
                return_tensors="pt", 
                max_length=kwargs.get('max_inp_length'),
            ).to(self.device)
        else:
            # MiniCPM-Llama3-V-2_5, ä»…æ¥å—å•å¼ ç…§ç‰‡é¢„æµ‹
            if len(prompts_lists) > 1:
                raise ValueError('`MiniCPM-Llama3-V-2_5` not support batch inference.')
            inputs = self.processor(
                prompts_lists[0], 
                input_images_lists[0], 
                return_tensors="pt", 
                max_length=kwargs.get('max_inp_length'),
            ).to(self.device)
            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'], dtype=bool)

        inputs.pop("image_sizes")
        history.append({'role': 'user', 'content': query, 'images': images})
        return inputs


# ==========================================================================================
# =======================                ç»Ÿä¸€Chatå…¥å£             ==========================
# ==========================================================================================
MAPPING = {
    'minicpmv': MiniCPMV
}

class ChatV:
    """
    éƒ¨ç½²ç±»ä¼¼OpenAiçš„api serverç«¯

    ### åŸºç¡€å‚æ•°
    :param checkpoint_path: str, æ¨¡å‹æ‰€åœ¨çš„æ–‡ä»¶å¤¹åœ°å€
    :param precision: bool, ç²¾åº¦, 'double', 'float', 'half', 'float16', 'bfloat16'
    :param quantization_config: dict, æ¨¡å‹é‡åŒ–ä½¿ç”¨åˆ°çš„å‚æ•°, eg. {'quantization_method':'cpm_kernels', 'quantization_bit':8}
    :param generation_config: dict, genrerateä½¿ç”¨åˆ°çš„å‚æ•°, eg. {'mode':'random_sample', 'max_length':2048, 'default_rtype':'logits', 'use_states':True}
        - bos_token_id: int, è§£ç ä½¿ç”¨çš„èµ·å§‹token_id, ä¸åŒé¢„è®­ç»ƒæ¨¡å‹è®¾ç½®å¯èƒ½ä¸ä¸€æ ·
        - eos_token_id: int/tuple/list, è§£ç ä½¿ç”¨çš„ç»“æŸtoken_id, ä¸åŒé¢„è®­ç»ƒæ¨¡å‹è®¾ç½®å¯èƒ½ä¸ä¸€æ ·, é»˜è®¤ç»™çš„-1(çœŸå®åœºæ™¯ä¸­ä¸å­˜åœ¨, è¡¨ç¤ºè¾“å‡ºåˆ°max_length)
        - max_new_tokens: int, æœ€å¤§è§£ç é•¿åº¦
        - min_new_tokens: int, æœ€å°è§£ç é•¿åº¦, é»˜è®¤ä¸º1
        - max_length: int, æœ€å¤§æ–‡æœ¬é•¿åº¦
        - pad_token_id: int, pad_id, åœ¨batchè§£ç æ—¶å€™ä½¿ç”¨
        - pad_mode: str, paddingåœ¨å‰é¢è¿˜æ˜¯åé¢, preæˆ–è€…post
        - device: str, é»˜è®¤ä¸º'cpu'
        - n: int, random_sampleæ—¶å€™è¡¨ç¤ºç”Ÿæˆçš„ä¸ªæ•°; beam_searchæ—¶è¡¨ç¤ºæŸå®½
        - top_k: int, è¿™é‡Œçš„topkæ˜¯æŒ‡ä»…ä¿ç•™topkçš„å€¼ (ä»…åœ¨top_kä¸Šè¿›è¡Œæ¦‚ç‡é‡‡æ ·)
        - top_p: float, è¿™é‡Œçš„toppæ˜¯tokençš„æ¦‚ç‡é˜ˆå€¼è®¾ç½®(ä»…åœ¨å¤´éƒ¨top_pä¸Šè¿›è¡Œæ¦‚ç‡é‡‡æ ·)
        - temperature: float, æ¸©åº¦å‚æ•°, é»˜è®¤ä¸º1, è¶Šå°ç»“æœè¶Šç¡®å®š, è¶Šå¤§ç»“æœè¶Šå¤šæ ·
        - repetition_penalty: float, é‡å¤çš„æƒ©ç½šç³»æ•°, è¶Šå¤§ç»“æœè¶Šä¸é‡å¤
        - min_ends: int, æœ€å°çš„end_idçš„ä¸ªæ•°
    :param create_model_at_startup: bool, æ˜¯å¦åœ¨å¯åŠ¨çš„æ—¶å€™åŠ è½½æ¨¡å‹, é»˜è®¤ä¸ºTrue
    :param system: Optional[str]=None, æ¨¡å‹ä½¿ç”¨çš„systemä¿¡æ¯, ä»…éƒ¨åˆ†æ¨¡å‹å¯ç”¨, ä¸”openai apiæ ¼å¼çš„ä¸éœ€è¦è®¾ç½®è¯¥å‚æ•°

    ### æ¨¡å¼
    :param mode: å‘½ä»¤è¡Œ, web, apiæœåŠ¡æ¨¡å¼, Literal['raw', 'cli', 'gradio', 'streamlit', 'openai']
    :param template: ä½¿ç”¨çš„æ¨¡æ¿, ä¸€èˆ¬åœ¨bert4torch_config.jsonä¸­æ— éœ€å•ç‹¬è®¾ç½®, å¯è‡ªè¡ŒæŒ‡å®š

    ### openai apiå‚æ•°
    :param name: str, æ¨¡å‹åç§°
    :param route_api: str, apiçš„è·¯ç”±
    :param route_models: str, æ¨¡å‹åˆ—è¡¨çš„è·¯ç”±
    :param offload_when_nocall: str, æ˜¯å¦åœ¨ä¸€å®šæ—¶é•¿å†…æ— è°ƒç”¨å°±å¸è½½æ¨¡å‹ï¼Œå¯ä»¥å¸è½½åˆ°å†…å­˜å’Œdiskä¸¤ç§
    :param max_callapi_interval: int, æœ€é•¿è°ƒç”¨é—´éš”
    :param scheduler_interval: int, å®šæ—¶ä»»åŠ¡çš„æ‰§è¡Œé—´éš”
    :param api_keys: List[str], api keysçš„list

    ### Examples:
    ```python
    >>> from bert4torch.pipelines import Chat

    >>> checkpoint_path = "E:/data/pretrain_ckpt/glm/chatglm2-6b"
    >>> generation_config  = {'mode':'random_sample',
    ...                     'max_length':2048, 
    ...                     'default_rtype':'logits', 
    ...                     'use_states':True
    ...                     }
    >>> chat = Chat(checkpoint_path, generation_config=generation_config, mode='cli')
    >>> chat.run()
    ```
    """
    def __init__(self, 
                 # åŸºç±»ä½¿ç”¨
                 checkpoint_path:str, 
                 config_path:str=None,
                 precision:Literal['double', 'float', 'half', 'float16', 'bfloat16', None]=None, 
                 quantization_config:dict=None, 
                 generation_config:dict=None, 
                 create_model_at_startup:bool=True,
                 # cliå‚æ•°
                 system:str=None,
                 # openapiå‚æ•°
                 name:str='default', 
                 route_api:str='/chat/completions', 
                 route_models:str='/models', 
                 max_callapi_interval:int=24*3600, 
                 scheduler_interval:int=10*60, 
                 offload_when_nocall:Literal['cpu', 'disk']=None, 
                 api_keys:List[str]=None,
                 # æ¨¡å¼
                 mode:Literal['raw','gradio', 'streamlit', 'openai']='openai',
                 template: str=None,
                 **kwargs
                 ) -> None:
        pass

    def __new__(cls, *args, mode:Literal['raw', 'cli', 'gradio', 'streamlit', 'openai']='cli', **kwargs):
        # templateæŒ‡å®šä½¿ç”¨çš„æ¨¡æ¿
        if kwargs.get('template') is not None:
            template = kwargs.pop('template')
        else:
            config_path = kwargs['config_path'] if kwargs.get('config_path') is not None else args[0]
            config = json.load(open(get_config_path(config_path, allow_none=True)))
            template = config.get('template', config.get('model', config.get('model_type')))
        if template is None:
            raise ValueError('template/model/model_type not found in bert4torch_config.json')
        else:
            ChatTemplate = MAPPING[template]
            log_info_once(f'Chat pipeline use template=`{template}` and mode=`{mode}`')

        if mode == 'gradio':
            @add_start_docstrings(CHAT_START_DOCSTRING)
            class ChatDemo(ChatTemplate, ChatVWebGradio): pass
        elif mode == 'streamlit':
            @add_start_docstrings(CHAT_START_DOCSTRING)
            class ChatDemo(ChatTemplate, ChatVWebStreamlit): pass
        elif mode == 'openai':
            @add_start_docstrings(OPENAI_START_DOCSTRING)
            class ChatDemo(ChatTemplate, ChatVOpenaiApi): pass
        elif mode == 'raw':
            ChatDemo = ChatTemplate
        else:
            raise ValueError(f'Unsupported mode={mode}')
        return ChatDemo(*args, **kwargs)