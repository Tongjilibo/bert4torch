''' å¤§æ¨¡å‹èŠå¤©çš„pipelineè°ƒç”¨
ä¸»è¦åŠŸèƒ½ï¼š
1. å‘½ä»¤è¡Œè°ƒç”¨å„ä¸ªæ¨¡å‹demo
2. åˆ©ç”¨fastapiä¸ºå¤§æ¨¡å‹æ­å»ºopenaiæ ¼å¼çš„serverå’Œclientè°ƒç”¨
    Implements API for LLM in OpenAI's format. (https://platform.openai.com/docs/api-reference/chat)
    Usage: python openai_api.py
    Visit http://localhost:8000/docs for documents.
3. webç•Œé¢å¿«é€Ÿæ­å»ºdemo(gradio+streamlit)

# TODO: è®¾ç½®return_states=Trueæ—¶å€™ï¼Œå—åˆ°build_promptå½±å“ï¼Œå¾ˆéš¾ä¿è¯promptå®Œå…¨å¤ç°
è¿™é‡Œé‡‡ç”¨æ·»åŠ self.generation_config['states']['last_token']ï¼Œæ˜¯å› ä¸ºæ¨ç†å®Œæˆå¯èƒ½æ˜¯å› ä¸ºåˆ°è¾¾max_lengthï¼Œæœªå¿…æ˜¯é‡åˆ°äº†eos
'''

import torch
from typing import Union, Optional, List, Tuple, Literal, Dict
from bert4torch.pipelines.chat import ChatBase, ChatWebGradio, ChatWebStreamlit, ChatOpenaiApi, CHAT_START_DOCSTRING, OPENAI_START_DOCSTRING
from bert4torch.pipelines.chat import ChatCompletionRequest, ChatCompletionResponseChoice, ChatMessage, ChatCompletionResponse, ChatCompletionResponseStreamChoice, DeltaMessage
from bert4torch.models.qwen2_vl import process_vision_info
from bert4torch.models.qwen2_vl.vision_process import MIN_PIXELS, MAX_PIXELS
from bert4torch.snippets import (
    log_warn_once, 
    get_config_path, 
    log_info, 
    log_info_once,
    log_warn, 
    log_error,
    cuda_empty_cache,
    is_fastapi_available, 
    is_pydantic_available, 
    is_sseclient_available, 
    is_streamlit_available,
    is_package_available,
    has_chinese_char,
    add_start_docstrings,
    JsonConfig,
    is_transformers_available,
    inference_mode,
    sequence_padding
)
import time
import json
import copy
from PIL import Image
import inspect
import numpy as np
import os
import base64
from io import BytesIO


if is_fastapi_available():
    from fastapi import FastAPI, HTTPException, APIRouter, Depends
    from fastapi.security.http import HTTPAuthorizationCredentials, HTTPBearer
    from fastapi.middleware.cors import CORSMiddleware
else:
    class FastAPI: pass
    class HTTPAuthorizationCredentials: pass
    Depends, HTTPBearer = object, object

if is_pydantic_available():
    from pydantic import BaseModel, Field
else:
    BaseModel, Field = object, object

if is_streamlit_available():
    import streamlit as st
else:
    # é˜²æ­¢streamlitä¸å­˜åœ¨æ—¶å€™æŠ¥é”™
    import bert4torch.snippets as st
    st.cache_resource = st.delete_arguments

if is_transformers_available():
    from transformers import AutoProcessor

__all__ = [
    'ChatVBase',
    'MiniCPMV',
    "ChatV"
    ]

ImageType = Union[str, Image.Image, np.ndarray]
VedioType = Union[str, Image.Image, np.ndarray]

def trans_query_images_tolist(query, images):
    '''æŠŠqueryï¼Œimagesè½¬ä¸ºlist'''
    def trans_images_to_Image(images:Union[ImageType, List[ImageType], List[List[ImageType]]]):
        '''æŠŠå„ç§ç±»å‹çš„imagesè½¬åŒ–ä¸ºImage.Imageæ ¼å¼'''
        if isinstance(images, str):
            images = Image.open(images).convert('RGB')
        elif isinstance(images, np.ndarray):
            images = Image.fromarray(images)
        elif isinstance(images, List) and all([isinstance(image, (str, Image.Image, np.ndarray)) for image in images]):
            images = [trans_images_to_Image(image) for image in images]
        elif isinstance(images, List) and all([isinstance(image, List) for image in images]):
            images = [trans_images_to_Image(image) for image in images]
        return images

    images = trans_images_to_Image(images)

    if isinstance(query, str):
        query = [query]
        if isinstance(images, Image.Image):
            # æé—®å•å¼ å›¾ç‰‡
            images = [images]
        elif isinstance(images, List) and all([isinstance(image, Image.Image) for image in images]):
            # åŒæ—¶æé—®å¤šå¼ å›¾ç‰‡
            images = [images]
        elif images is None:
            images = [images]
    elif isinstance(query, List) and all([isinstance(query, str) for query in query]):
        if isinstance(images, Image.Image):
            # å¤šæ¬¡æé—®å•å¼ å›¾ç‰‡
            images = [images] * len(query)
        elif isinstance(images, List) and all([isinstance(image, Image.Image) for image in images]):
            # å„è‡ªæé—®å•å¼ å›¾ç‰‡
            pass
        elif isinstance(images, List) and all([isinstance(image, List) for image in images]) and \
            all([isinstance(i, Image.Image) for image in images for i in image]):
            # å„è‡ªåŒæ—¶æé—®å¤šå¼ å›¾ç‰‡
            pass
    return query, images

class ChatVBase(ChatBase):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.return_tensor_from_build_prompt = True
        self.processor = AutoProcessor.from_pretrained(self.checkpoint_path, trust_remote_code=True)

    def generate(self, *args, **kwargs):
        '''baseæ¨¡å‹ä½¿ç”¨'''
        return self.model.generate(*args, **kwargs)

    def build_prompt(self, *args, **kwargs) -> str:
        raise NotImplementedError

    @inference_mode()
    def chat(self, query:Union[str, List[str]], images:Union[ImageType, List[ImageType]]=None, 
             vedios=None, history:List[dict]=None, functions:List[dict]=None, **kwargs) -> Union[str, List[str]]:
        '''chatæ¨¡å‹ä½¿ç”¨, é…åˆå¯¹è¯æ¨¡æ¿ä½¿ç”¨'''
        history = history or []

        if isinstance(query, str) or self.return_tensor_from_build_prompt:
            # å•æ¡è¾“å…¥ï¼Œæˆ–åœ¨build_prompté˜¶æ®µå³ç»„å»ºäº†batchçš„tensor
            inputs:Dict[torch.Tensor] = self.build_prompt(query, images, vedios, history, functions, **kwargs)
            response = self.model.generate(**inputs, **self.generation_config)
            if isinstance(response, str):
                # ç”Ÿæˆå•æ¡è¾“å‡º
                return self.process_response_history(response, history=history)
            elif isinstance(response, list):
                # ä¸ºå•æ¡queryç”Ÿæˆå¤šæ¡response
                return [self.process_response_history(resp, history=copy.deepcopy(history)) for resp in response]
            else:
                raise TypeError(f'`response` type={type(response)} which is not supported')
            
        elif isinstance(query, list):
            # å¤šæ¡è¾“å…¥
            history_copy = [copy.deepcopy(history) for _ in query]
            images = [images] * len(query) if images is None or isinstance(images, (str, Image.Image, np.ndarray)) else images
            vedios = [vedios] * len(query) if vedios is None or isinstance(vedios, (str, Image.Image, np.ndarray)) else vedios
            inputs:List[Dict[torch.Tensor]] = [self.build_prompt(q, img, ved, hist, functions, **kwargs) for q, img, ved, hist in zip(query, images, vedios, history_copy)]
            response = self.model.generate(inputs, **self.generation_config)
            return [self.process_response_history(r, history=hist) for r, hist in zip(response, history_copy)]
        else:
            raise TypeError(f'Args `query` type={type(query)} which is not supported')


class ChatVWebGradio(ChatWebGradio):
    '''éœ€è¦æ·»åŠ ä¸€ä¸ªå›¾ç‰‡çš„ä¸Šä¼ '''
    @staticmethod
    def get_image_vedio(chatbot):
        def _is_video_file(filename):
            video_extensions = ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.mpeg']
            return any(filename.lower().endswith(ext) for ext in video_extensions)
        
        input_image, input_vedio = None, None
        if chatbot and isinstance(chatbot[-1][0], tuple) and os.path.isfile(chatbot[-1][0][0]):
            if _is_video_file(chatbot[-1][0][0]):
                input_vedio = chatbot[-1][0][0]  # è§†é¢‘
            else:
                input_image = chatbot[-1][0][0]  # å›¾ç‰‡
        return input_image, input_vedio
            

    def _stream_predict(self, query, chatbot, history, max_length, top_p, temperature, repetition_penalty, system, functions):
        '''æµå¼ç”Ÿæˆ'''
        self.set_generation_config(max_length, top_p, temperature, repetition_penalty)
        input_image, input_vedio = self.get_image_vedio(chatbot)
        chatbot.append((query, ""))
        self._set_system_functions(system, functions)
        input_kwargs = self.build_prompt(query, input_image, input_vedio, history, functions)
        for response in self.model.stream_generate(**input_kwargs, **self.generation_config):
            response = self.process_response_history(response, history)
            if history[-1].get('raw_content'):
                response = history[-1]['raw_content']
            if history[-1].get('function_call'):
                response += f"\n\nFunctionï¼š{history[-1]['function_call']}"
            chatbot[-1] = (query, response)
            yield chatbot, history
        cuda_empty_cache()  # æ¸…ç†æ˜¾å­˜

    def run(self, host:str=None, port:int=None, **launch_configs):

        def add_file(chatbot, file):
            chatbot = chatbot if chatbot is not None else []
            chatbot = chatbot + [((file.name,), None)]
            return chatbot

        with self.gr.Blocks() as demo:
            self.gr.HTML("""<h1 align="center">Chabot Gradio Demo</h1>""")
            with self.gr.Row():
                with self.gr.Column(scale=4):
                    chatbot = self.gr.Chatbot(height=500)
                    with self.gr.Column(scale=12):
                        query = self.gr.Textbox(show_label=False, placeholder="Input...", lines=10, max_lines=10) # .style(container=False)
                    with self.gr.Row():
                        addfile_btn = self.gr.UploadButton('ğŸ“ Upload', file_types=['image', 'video'])
                        submitBtn = self.gr.Button("ğŸš€ Submit", variant="primary")
                        regen_btn = self.gr.Button('ğŸ¤”ï¸ Regenerate')
                        emptyBtn = self.gr.Button("ğŸ§¹ Clear History")

                with self.gr.Column(scale=1):
                    max_length = self.gr.Slider(0, self.max_length, value=self.max_length, step=1.0, label="max_length", interactive=True)
                    top_p = self.gr.Slider(0, 1, value=self.generation_config.get('top_p', 1.0), step=0.01, label="top_p", interactive=True)
                    temperature = self.gr.Slider(0, self.max_temperature, value=self.generation_config.get('temperature', 1.0), step=0.1, label="temperature", interactive=True)
                    repetition_penalty = self.gr.Slider(0, self.max_repetition_penalty, value=self.generation_config.get('repetition_penalty', 1.0), step=0.1, label="repetition_penalty", interactive=True)
                    system = self.gr.Textbox(label='System Prompt (If exists)', lines=6, max_lines=6)
                    functions = self.gr.Textbox(label='Functions Json Format (If exists)', lines=6, max_lines=6)
                
            history = self.gr.State([])
            _input_tuple = [query, chatbot, history, max_length, top_p, temperature, repetition_penalty, system, functions]
            addfile_btn.upload(add_file, [chatbot, addfile_btn], [chatbot], show_progress=True)
            submitBtn.click(self._stream_predict, _input_tuple, [chatbot, history], show_progress=True)
            submitBtn.click(self.reset_user_input, [], [query])
            regen_btn.click(self.regenerate, _input_tuple, [chatbot, history], show_progress=True)
            emptyBtn.click(self.reset_state, outputs=[chatbot, history], show_progress=True)

        demo.queue().launch(server_name = launch_configs.pop('server_name', host), 
                            server_port = launch_configs.pop('server_port', port), 
                            **launch_configs)


class ChatVWebStreamlit(ChatWebStreamlit):
    def run(self, debug:bool=False):
        def check_img_in_history(history, tgt_img):
            for i, message in enumerate(history):
                if message.get('images') is not None:
                    for img in message['images']:
                        if isinstance(img, list):
                            for i in img:
                                if i == tgt_img:
                                    return True
                        elif img == tgt_img:
                            return True
            return False

        if "history" not in st.session_state:
            st.session_state.history = []
        if "states" not in st.session_state:
            st.session_state.states = None

        max_length = st.sidebar.slider("max_length", 0, self.max_length, self.max_length, step=1)
        top_p = st.sidebar.slider("top_p", 0.0, 1.0, self.generation_config.get('top_p', 1.0), step=0.01)
        temperature = st.sidebar.slider("temperature", 0.0, self.max_temperature, self.generation_config.get('temperature', 1.0), step=0.01)
        repetition_penalty = st.sidebar.slider("repetition_penalty", 0.0, self.max_repetition_penalty, self.generation_config.get('repetition_penalty', 1.0), step=0.1)
        buttonClean = st.sidebar.button("Clear history", key="clean")
        if buttonClean:
            st.session_state.history = []
            st.session_state.states = None
            cuda_empty_cache()
            st.rerun()
        
        # Select mode
        selected_mode = st.sidebar.selectbox("Select Mode", ["Text", "Images", "Video"])

        # Supported image file extensions
        image_type = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']

        for i, message in enumerate(st.session_state.history):
            role = message['role']
            if role not in {'user', 'assistant'}:
                continue
            if role == 'user':
                if message.get('images') is not None:
                    for img in message['images']:
                        if isinstance(img, list):
                            if not img:
                                continue
                            with st.chat_message(name=role, avatar=role):
                                for i in img:
                                    st.image(i, caption='User Uploaded Image', width=512, use_column_width=False)
                        else:
                            with st.chat_message(name=role, avatar=role):
                                st.image(img, caption='User Uploaded Image', width=512, use_column_width=False)
                if message.get('vedio') is not None:
                    with st.chat_message(name=role, avatar=role):
                        st.video(message['vedio'], format="video/mp4", loop=False, autoplay=False, muted=True) 
            with st.chat_message(name=role, avatar=role):
                st.markdown(message.get('raw_content', message['content']))
        
        images = []
        if selected_mode == "Images":
            # Multiple Images Mode
            uploaded_image_list = st.sidebar.file_uploader("Upload Images", key=2, type=image_type, accept_multiple_files=True)
            if uploaded_image_list is not None:
                for img in uploaded_image_list:
                    # st.image(img, caption='User Uploaded Image', width=512, use_column_width=False)
                    # åˆ¤æ–­imgæ˜¯å¦åœ¨å†å²ä¸­
                    img_numpy = Image.open(img).convert('RGB')
                    if not check_img_in_history(st.session_state.history, img_numpy):
                        with st.chat_message(name='user', avatar='user'):
                            st.image(img, caption='User Uploaded Image', width=512, use_column_width=False)
                        images.append(img_numpy)

        # Supported video format suffixes
        video_type = ['.mp4', '.mkv', '.mov', '.avi', '.flv', '.wmv', '.webm', '.m4v']
        videos = []
        # Tip: You can use the command `streamlit run ./web_demo_streamlit-minicpmv2_6.py --server.maxUploadSize 1024`
        # to adjust the maximum upload size to 1024MB or larger files.
        # The default 200MB limit of Streamlit's file_uploader component might be insufficient for video-based interactions.
        # Adjust the size based on your GPU memory usage.

        if selected_mode == "Video":
            # å•ä¸ªè§†é¢‘æ¨¡æ€
            uploaded_video = st.sidebar.file_uploader("Upload a single video file", key=3, type=video_type,
                                                    accept_multiple_files=False)
            if uploaded_video is not None:
                # st.video(uploaded_video, format="video/mp4", loop=False, autoplay=False, muted=True)
                with st.chat_message(name='user', avatar='user'):
                    st.video(uploaded_video, format="video/mp4", loop=False, autoplay=False, muted=True)
                videos.append(uploaded_video)

                uploaded_video_path = os.path.join(".\\uploads", uploaded_video.name)
                with open(uploaded_video_path, "wb") as vf:
                    vf.write(uploaded_video.getvalue())
                
        system = st.sidebar.text_area(
            label="System Prompt (If exists)",
            height=200,
            value="",
        )
        functions = st.sidebar.text_area(
            label="Functions Json Format (If exists)",
            height=200,
            value="",
        )

        try:
            if functions is not None and functions.strip() != '':
                functions = json.loads(functions)
            else:
                functions = None
        except json.JSONDecodeError:
            functions = None
            log_warn('Functions implement not json format')

        if system is not None and system.strip() != '':
            self.system = system
        
        with st.chat_message(name="user", avatar="user"):
            input_placeholder = st.empty()
        with st.chat_message(name="assistant", avatar="assistant"):
            message_placeholder = st.empty()

        query = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")
        if query:
            if query.strip() == "":
                st.warning('Input message could not be empty!', icon="âš ï¸")
            else:
                input_placeholder.markdown(query)
                history = st.session_state.history
                states = st.session_state.states
                self.generation_config['max_length'] = max_length
                self.generation_config['top_p'] = top_p
                self.generation_config['temperature'] = temperature
                self.generation_config['repetition_penalty'] = repetition_penalty
                self.generation_config['states'] = states

                if debug:
                    log_info(f'History before generate: {history}')
                input_kwargs = self.build_prompt(query, images, videos, history, functions)
                for response in self.model.stream_generate(**input_kwargs, **self.generation_config):
                    response = self.process_response_history(response, history)
                    message_placeholder.markdown(history[-1].get('raw_content', response))
                st.session_state.history = history
                if debug:
                    log_info(f'History after generate: {history}')
                st.session_state.states = self.generation_config.get('states')


class ChatVOpenaiApi(ChatOpenaiApi):
    async def create_chat_completion(self, request: ChatCompletionRequest):
        if request.model != self.name:
            raise HTTPException(status_code=404, detail=f"Invalid model: request.model:{request.model} != self.name:{self.name}")
        if request.messages[-1].role != self.role_user:  # æœ€åä¸€æ¡msgçš„roleå¿…é¡»æ˜¯user
            raise HTTPException(status_code=400, detail=f"Invalid request: messages last role shold be {self.role_user}")

        if request.temperature:
            self.generation_config['temperature'] = request.temperature
        if request.top_p:
            self.generation_config['top_p'] = request.top_p
        if request.top_k:
            self.generation_config['top_k'] = request.top_k
        if request.max_length:
            self.generation_config['max_length'] = request.max_length
        if request.repetition_penalty:
            self.generation_config['repetition_penalty'] = request.repetition_penalty

        content = request.messages[-1].content
        def get_query_images(content):
            if isinstance(content, str):
                query = content
                images = None
            else:  # dict
                query = [i for i in content if i['type']=='text'][0]['content']
                images = [Image.open(BytesIO(base64.b64decode(i['url']))).convert('RGB') for i in content if i['type']=='image_url']
            return query, images
        query, images = get_query_images(content)
        history = []
        for item in request.messages[:-1]:
            item_query, item_images = get_query_images(item.content)
            history.append({'role': item.role, 'content': item_query, 'images': item_images})
        input_kwargs = self.build_prompt(query, images, None, history, request.functions)
        
        if self.offload_when_nocall is None:
            self.model = self.build_model()
        else:
            with self.lock:
                self.model = self.build_model()
            self.last_callapi_timestamp = time.time()

        # æµå¼è¾“å‡º
        if request.stream:
            generate = self.predict(input_kwargs, request.model, history)
            return self.EventSourceResponse(generate, media_type="text/event-stream")
        
        # éæµå¼è¾“å‡º
        else:
            response = self.model.generate(**input_kwargs, **self.generation_config)
            response = self.process_response_history(response, history)
            function_call = history[-1].get('function_call', None)
            choice_data = ChatCompletionResponseChoice(
                index=0,
                message=ChatMessage(role=self.role_assistant, content=response, function_call=function_call),
                finish_reason= "function_call" if function_call is not None else "stop"
            )

            return ChatCompletionResponse(model=request.model, choices=[choice_data], object="chat.completion")

    async def predict(self, input_kwargs: dict, model_id: str, history:list):
        choice_data = ChatCompletionResponseStreamChoice(
            index=0,
            delta=DeltaMessage(role=self.role_assistant),
            finish_reason=None
        )
        chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object="chat.completion.chunk")
        yield "{}".format(chunk.model_dump_json(exclude_unset=True))

        current_length = 0

        for new_response in self.model.stream_generate(**input_kwargs, **self.generation_config):
            if len(new_response) == current_length:
                continue

            self.process_response_history(new_response, history)
            new_text = new_response[current_length:]
            current_length = len(new_response)

            function_call = history[-1].get('function_call', None)
            choice_data = ChatCompletionResponseStreamChoice(
                index=0,
                delta=DeltaMessage(content=new_text, function_call=function_call),
                finish_reason=None
            )
            chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object="chat.completion.chunk")
            yield "{}".format(chunk.model_dump_json(exclude_unset=True))

        function_call = history[-1].get('function_call', None)
        choice_data = ChatCompletionResponseStreamChoice(
            index=0,
            delta=DeltaMessage(function_call=function_call),
            finish_reason= "function_call" if function_call is not None else "stop"
        )
        chunk = ChatCompletionResponse(model=model_id, choices=[choice_data], object="chat.completion.chunk")
        yield "{}".format(chunk.model_dump_json(exclude_unset=True))
        yield '[DONE]'


class MiniCPMV(ChatVBase):
    def build_prompt(
            self,
            queries: Union[str, list], 
            images: Union[Image.Image, List[Image.Image]]=None, 
            vedios=None, history: List[Dict]=None, 
            functions:List[dict]=None,
            **kwargs):
        '''
        history: [
                    {'role': 'user', 'content': 'å›¾ç‰‡ä¸­æè¿°çš„æ˜¯ä»€ä¹ˆ', 'images': [PIL.Image.Image]},
                    {'role': 'assistant', 'content': 'è¯¥å›¾ç‰‡ä¸­æè¿°äº†ä¸€ä¸ªå°ç”·å­©åœ¨è¸¢è¶³çƒ'},
                 ]

        |   queries   |        images      |     comment      |
        |   -------   |      --------      |    ---------     |
        |     str     |        Image       |    æé—®å•å¼ å›¾ç‰‡   |
        |     str     |     List[Image]    |  åŒæ—¶æé—®å¤šå¼ å›¾ç‰‡  |
        |  List[str]  |        Image       |  å¤šæ¬¡æé—®å•å¼ å›¾ç‰‡  |
        |  List[str]  |     List[Image]    |  å„è‡ªæé—®å•å¼ å›¾ç‰‡  |
        |  List[str]  |  List[List[Image]] |å„è‡ªåŒæ—¶æé—®å¤šå¼ å›¾ç‰‡|
        '''
        queries, images = trans_query_images_tolist(queries, images)

        assert len(queries) == len(images), "The batch dim of query and images should be the same."        
        assert self.model.config.query_num == self.processor.image_processor.image_feature_size, "These two values should be the same. Check `config.json` and `preprocessor_config.json`."
        assert self.model.config.patch_size == self.processor.image_processor.patch_size, "These two values should be the same. Check `config.json` and `preprocessor_config.json`."
        # assert self.model.config.use_image_id == self.processor.image_processor.use_image_id, "These two values should be the same. Check `config.json` and `preprocessor_config.json`."
        assert self.model.config.slice_config.max_slice_nums == self.processor.image_processor.max_slice_nums, "These two values should be the same. Check `config.json` and `preprocessor_config.json`."
        # assert self.model.config.slice_mode == self.processor.image_processor.slice_mode, "These two values should be the same. Check `config.json` and `preprocessor_config.json`."

        # å¤„ç†history
        history_images = []
        history_copy = copy.deepcopy(history) or []
        for i, hist in enumerate(history_copy):
            role = hist["role"]
            content = hist["content"]
            assert role in ["user", "assistant"]
            if i == 0:
                assert role == "user", "The role of first msg should be user"
            
            if 'images' not in hist:
                continue
            if isinstance(hist["images"], Image.Image):
                hist["images"] = [hist["images"]]
            hist["content"] = "(<image>./</image>)\n"*len(hist["images"]) + content
            history_images.extend(hist["images"])

        prompts_lists = []
        input_images_lists = []
        for q, image in zip(queries, images):
            copy_msgs = copy.deepcopy(history_copy) or []
            if image is None:
                image = []
            elif isinstance(image, Image.Image):
                image = [image]
            content = "(<image>./</image>)\n"*len(image) + q
            copy_msgs.append({'role': 'user', 'content': content})

            if kwargs.get('system_prompt'):
                sys_msg = {'role': 'system', 'content': kwargs.get('system_prompt')}
                copy_msgs = [sys_msg] + copy_msgs        

            prompts_lists.append(self.processor.tokenizer.apply_chat_template(copy_msgs, tokenize=False, add_generation_prompt=True))
            input_images_lists.append(history_images + image)
        
        if 'max_slice_nums' in inspect.signature(self.processor).parameters:
            # MiniCPM-V-2_6
            inputs = self.processor(
                prompts_lists, 
                input_images_lists, 
                max_slice_nums=kwargs.get('max_slice_nums'),
                use_image_id=kwargs.get('use_image_id'),
                return_tensors="pt", 
                max_length=kwargs.get('max_inp_length'),
            ).to(self.device)
        else:
            # MiniCPM-Llama3-V-2_5, ä»…æ¥å—å•å¼ ç…§ç‰‡é¢„æµ‹
            if len(prompts_lists) > 1:
                raise ValueError('`MiniCPM-Llama3-V-2_5` not support batch inference.')
            inputs = self.processor(
                prompts_lists[0], 
                input_images_lists[0], 
                return_tensors="pt", 
                max_length=kwargs.get('max_inp_length'),
            ).to(self.device)
            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'], dtype=bool)

        inputs.pop("image_sizes")
        history.append({'role': 'user', 'content': queries[0]})
        if all([i is not None for i in images]):
            history[-1]['images'] = images
        return inputs


class Qwen2VL(ChatVBase):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.min_pixels = kwargs.get('min_pixels', MIN_PIXELS)
        self.max_pixels = kwargs.get('max_pixels', MAX_PIXELS)
        log_warn_once('Please set `max_pixels` smaller when CUDA out_of_memory eccured')

    def build_prompt(
            self,
            queries: Union[str, List[str]], 
            images: Union[Image.Image, List[Image.Image], List[List[Image.Image]]]=None, 
            vedios=None,
            history: List[Dict]=None, 
            functions:List[dict]=None,
            **kwargs
        ):
        if hasattr(self, 'system') and not history:
            history.append({'role': 'system', 'content': self.system})
        queries, images = trans_query_images_tolist(queries, images)

        all_messages = []
        for query, image in zip(queries, images):
            messages = (copy.deepcopy(history) or [] or []) + [
                {
                    "role": "user",
                    "content": [{"type": "text", "text": query}]
                }]
            if image is None:
                pass
            elif isinstance(image, list):
                messages[-1]['content'] = [{"type": "image", "image": i, "min_pixels":self.min_pixels, 
                                            "max_pixels": self.max_pixels} for i in image] + messages[-1]['content']
            else:
                messages[-1]['content'] = [{"type": "image", "image": image, "min_pixels":self.min_pixels, 
                                            "max_pixels": self.max_pixels}] + messages[-1]['content']
            all_messages.append(messages)

        text = self.processor.apply_chat_template(all_messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(all_messages)
        inputs = self.processor(
            text=text,
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        ).to(self.device)
        history.append({'role': 'user', 'content': queries[0]})
        if all([i is not None for i in images]):
            history[-1]['images'] = images
        return inputs


# ==========================================================================================
# =======================                ç»Ÿä¸€Chatå…¥å£             ==========================
# ==========================================================================================
MAPPING = {
    'minicpmv': MiniCPMV,
    'qwen2_vl': Qwen2VL
}

class ChatV:
    """
    éƒ¨ç½²ç±»ä¼¼OpenAiçš„api serverç«¯

    ### åŸºç¡€å‚æ•°
    :param checkpoint_path: str, æ¨¡å‹æ‰€åœ¨çš„æ–‡ä»¶å¤¹åœ°å€
    :param precision: bool, ç²¾åº¦, 'double', 'float', 'half', 'float16', 'bfloat16'
    :param quantization_config: dict, æ¨¡å‹é‡åŒ–ä½¿ç”¨åˆ°çš„å‚æ•°, eg. {'quantization_method':'cpm_kernels', 'quantization_bit':8}
    :param generation_config: dict, genrerateä½¿ç”¨åˆ°çš„å‚æ•°, eg. {'mode':'random_sample', 'max_length':2048, 'default_rtype':'logits', 'use_states':True}
        - bos_token_id: int, è§£ç ä½¿ç”¨çš„èµ·å§‹token_id, ä¸åŒé¢„è®­ç»ƒæ¨¡å‹è®¾ç½®å¯èƒ½ä¸ä¸€æ ·
        - eos_token_id: int/tuple/list, è§£ç ä½¿ç”¨çš„ç»“æŸtoken_id, ä¸åŒé¢„è®­ç»ƒæ¨¡å‹è®¾ç½®å¯èƒ½ä¸ä¸€æ ·, é»˜è®¤ç»™çš„-1(çœŸå®åœºæ™¯ä¸­ä¸å­˜åœ¨, è¡¨ç¤ºè¾“å‡ºåˆ°max_length)
        - max_new_tokens: int, æœ€å¤§è§£ç é•¿åº¦
        - min_new_tokens: int, æœ€å°è§£ç é•¿åº¦, é»˜è®¤ä¸º1
        - max_length: int, æœ€å¤§æ–‡æœ¬é•¿åº¦
        - pad_token_id: int, pad_id, åœ¨batchè§£ç æ—¶å€™ä½¿ç”¨
        - pad_mode: str, paddingåœ¨å‰é¢è¿˜æ˜¯åé¢, preæˆ–è€…post
        - device: str, é»˜è®¤ä¸º'cpu'
        - n: int, random_sampleæ—¶å€™è¡¨ç¤ºç”Ÿæˆçš„ä¸ªæ•°; beam_searchæ—¶è¡¨ç¤ºæŸå®½
        - top_k: int, è¿™é‡Œçš„topkæ˜¯æŒ‡ä»…ä¿ç•™topkçš„å€¼ (ä»…åœ¨top_kä¸Šè¿›è¡Œæ¦‚ç‡é‡‡æ ·)
        - top_p: float, è¿™é‡Œçš„toppæ˜¯tokençš„æ¦‚ç‡é˜ˆå€¼è®¾ç½®(ä»…åœ¨å¤´éƒ¨top_pä¸Šè¿›è¡Œæ¦‚ç‡é‡‡æ ·)
        - temperature: float, æ¸©åº¦å‚æ•°, é»˜è®¤ä¸º1, è¶Šå°ç»“æœè¶Šç¡®å®š, è¶Šå¤§ç»“æœè¶Šå¤šæ ·
        - repetition_penalty: float, é‡å¤çš„æƒ©ç½šç³»æ•°, è¶Šå¤§ç»“æœè¶Šä¸é‡å¤
        - min_ends: int, æœ€å°çš„end_idçš„ä¸ªæ•°
    :param create_model_at_startup: bool, æ˜¯å¦åœ¨å¯åŠ¨çš„æ—¶å€™åŠ è½½æ¨¡å‹, é»˜è®¤ä¸ºTrue
    :param system: Optional[str]=None, æ¨¡å‹ä½¿ç”¨çš„systemä¿¡æ¯, ä»…éƒ¨åˆ†æ¨¡å‹å¯ç”¨, ä¸”openai apiæ ¼å¼çš„ä¸éœ€è¦è®¾ç½®è¯¥å‚æ•°

    ### æ¨¡å¼
    :param mode: å‘½ä»¤è¡Œ, web, apiæœåŠ¡æ¨¡å¼, Literal['raw', 'cli', 'gradio', 'streamlit', 'openai']
    :param template: ä½¿ç”¨çš„æ¨¡æ¿, ä¸€èˆ¬åœ¨bert4torch_config.jsonä¸­æ— éœ€å•ç‹¬è®¾ç½®, å¯è‡ªè¡ŒæŒ‡å®š

    ### openai apiå‚æ•°
    :param name: str, æ¨¡å‹åç§°
    :param route_api: str, apiçš„è·¯ç”±
    :param route_models: str, æ¨¡å‹åˆ—è¡¨çš„è·¯ç”±
    :param offload_when_nocall: str, æ˜¯å¦åœ¨ä¸€å®šæ—¶é•¿å†…æ— è°ƒç”¨å°±å¸è½½æ¨¡å‹ï¼Œå¯ä»¥å¸è½½åˆ°å†…å­˜å’Œdiskä¸¤ç§
    :param max_callapi_interval: int, æœ€é•¿è°ƒç”¨é—´éš”
    :param scheduler_interval: int, å®šæ—¶ä»»åŠ¡çš„æ‰§è¡Œé—´éš”
    :param api_keys: List[str], api keysçš„list

    ### Examples:
    ```python
    >>> from bert4torch.pipelines import Chat

    >>> checkpoint_path = "E:/data/pretrain_ckpt/glm/chatglm2-6b"
    >>> generation_config  = {'mode':'random_sample',
    ...                     'max_length':2048, 
    ...                     'default_rtype':'logits', 
    ...                     'use_states':True
    ...                     }
    >>> chat = Chat(checkpoint_path, generation_config=generation_config, mode='cli')
    >>> chat.run()
    ```
    """
    def __init__(self, 
                 # åŸºç±»ä½¿ç”¨
                 checkpoint_path:str, 
                 config_path:str=None,
                 precision:Literal['double', 'float', 'half', 'float16', 'bfloat16', None]=None, 
                 quantization_config:dict=None, 
                 generation_config:dict=None, 
                 create_model_at_startup:bool=True,
                 # cliå‚æ•°
                 system:str=None,
                 # openapiå‚æ•°
                 name:str='default', 
                 route_api:str='/chat/completions', 
                 route_models:str='/models', 
                 max_callapi_interval:int=24*3600, 
                 scheduler_interval:int=10*60, 
                 offload_when_nocall:Literal['cpu', 'disk']=None, 
                 api_keys:List[str]=None,
                 # æ¨¡å¼
                 mode:Literal['raw','gradio', 'streamlit', 'openai']='raw',
                 template: str=None,
                 **kwargs
                 ) -> None:
        pass

    def __new__(cls, *args, mode:Literal['raw', 'gradio', 'streamlit', 'openai']='raw', **kwargs):
        # templateæŒ‡å®šä½¿ç”¨çš„æ¨¡æ¿
        if kwargs.get('template') is not None:
            template = kwargs.pop('template')
        else:
            config_path = kwargs['config_path'] if kwargs.get('config_path') is not None else args[0]
            config = json.load(open(get_config_path(config_path, allow_none=True)))
            template = config.get('template', config.get('model', config.get('model_type')))
        if template is None:
            raise ValueError('template/model/model_type not found in bert4torch_config.json')
        else:
            ChatTemplate = MAPPING[template]
            log_info_once(f'Chat pipeline use template=`{template}` and mode=`{mode}`')

        if mode == 'gradio':
            @add_start_docstrings(CHAT_START_DOCSTRING)
            class ChatDemo(ChatTemplate, ChatVWebGradio): pass
        elif mode == 'streamlit':
            @add_start_docstrings(CHAT_START_DOCSTRING)
            class ChatDemo(ChatTemplate, ChatVWebStreamlit): pass
        elif mode == 'openai':
            @add_start_docstrings(OPENAI_START_DOCSTRING)
            class ChatDemo(ChatTemplate, ChatVOpenaiApi): pass
        elif mode == 'raw':
            ChatDemo = ChatTemplate
        else:
            raise ValueError(f'Unsupported mode={mode}')
        return ChatDemo(*args, **kwargs)